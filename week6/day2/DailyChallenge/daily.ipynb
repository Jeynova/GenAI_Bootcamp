{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c9e0c9",
   "metadata": {},
   "source": [
    "\n",
    "Fine-Tuning GPT-2 for SMS Spam\n",
    "\n",
    "Last Updated: May 10th, 2025\n",
    "\n",
    "Daily Challenge: Fine-Tuning GPT-2 for SMS Spam Classification (Legacy transformers API)\n",
    "\n",
    "\n",
    "In this daily challenge, you’ll fine-tune a pre-trained GPT-2 model to classify SMS messages as spam or ham (not spam). We’ll work through loading the dataset, inspecting its schema, tokenizing examples, adapting to an older transformers version, and running training and evaluation with the classic do_train/do_eval flags.\n",
    "\n",
    "\n",
    "👩‍🏫 👩🏿‍🏫 What You’ll learn\n",
    "\n",
    "    How to load and explore a custom text-classification dataset\n",
    "    Inspecting and aligning column names for tokenization\n",
    "    Tokenizing text for GPT-2 (with its peculiar padding setup)\n",
    "    Initializing GPT2ForSequenceClassification\n",
    "    Defining and computing multiple evaluation metrics\n",
    "    Configuring TrainingArguments for transformers < 4.4 (using do_train, eval_steps, etc.)\n",
    "    Running fine-tuning with Trainer and interpreting results\n",
    "    Common pitfalls when using legacy APIs\n",
    "\n",
    "\n",
    "🛠️ What you will create\n",
    "\n",
    "By the end of this challenge, you will have built:\n",
    "\n",
    "    A tokenized SMS dataset compatible with GPT-2’s requirements, including custom padding and truncation.\n",
    "    A fine-tuned GPT2ForSequenceClassification model that can accurately label incoming SMS messages as spam or ham.\n",
    "    A complete training pipeline using the legacy do_train/do_eval flags in TrainingArguments, with periodic checkpointing, logging, and evaluation.\n",
    "    A set of evaluation metrics (accuracy, precision, recall, F1) computed at each validation step and summarized after training.\n",
    "    A reusable Jupyter notebook that ties everything together—from dataset loading and inspection, through model initialization and tokenization, to training, evaluation, and results interpretation.\n",
    "\n",
    "\n",
    "💼 Prerequisites\n",
    "\n",
    "    Python 3.7+\n",
    "    Installed packages: datasets, evaluate, transformers>=4.0.0,<4.4.0\n",
    "    Basic familiarity with Hugging Face’s datasets and transformers libraries\n",
    "    GitHub or Colab access for executing the notebook\n",
    "    A Hugging Face API and a WeightAndBiases API, for instructions on how to get it, click here.\n",
    "\n",
    "\n",
    "Task\n",
    "\n",
    "We will guide you through making a fine-tuning a GPT-2 model to classify SMS messages as spam or ham using an older version of transformers (<4.4). Follow the steps below and complete the “TODO” in the code.\n",
    "\n",
    "1. Setup : Install required packages datasets, evaluate and transformers[sentencepiece].\n",
    "\n",
    "%pip install --quiet datasets evaluate transformers[sentencepiece]\n",
    "\n",
    "\n",
    "2. Load & Inspect Dataset :\n",
    "\n",
    "from datasets import TODO #import load_dataset\n",
    "TODO # import pandas\n",
    "\n",
    "# Load the UCI SMS Spam dataset (sms_spam) from Hugging Face hub\n",
    "raw = TODO\n",
    "\n",
    "# We'll use 4,000 for train, 1,000 for validation\n",
    "train_ds = TODO\n",
    "val_ds   = TODO\n",
    "\n",
    "TODO  # print the features of the train dataset. It should show 'sms' and 'label'\n",
    "\n",
    "\n",
    "3. Tokenization :\n",
    "\n",
    "from transformers import TODO # import GPT2Tokenizer\n",
    "\n",
    "\n",
    "model_name = TODO #load the tokenize, we will use GPT2\n",
    "tokenizer  = TODO\n",
    "# GPT-2 has no pad token by default—set it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    # returns input_ids, attention_mask; keep max_length small for SMS\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_tok = TODO #apply the tokenization by loading the subset using .map function\n",
    "val_tok   = TODO #apply the tokenization by loading the subset using .map function\n",
    "\n",
    "\n",
    "4. Model Initialization\n",
    "\n",
    "import torch\n",
    "TODO  #import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained( # Load GPT-2 with sequence classification head\n",
    "    model_name,\n",
    "    num_labels=TODO,           # spam vs. ham\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "5. Metrics Definition\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = # apply the function used for accurracy but for precision\n",
    "recall    = # apply the function used for accurracy but for recall\n",
    "f1        = # apply the function used for accurracy but for F1\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"], \n",
    "        \"precision\": TODO, # apply the function used for accurracy but for precision\n",
    "        \"recall\":    TODO, # apply the function used for accurracy but for recall\n",
    "        \"f1\":        TODO # apply the function used for accurracy but for F1\n",
    "    }\n",
    "\n",
    "\n",
    "    In an imbalanced dataset like SMS spam (often more “ham” than “spam”), why is it important to track precision and recall alongside accuracy?\n",
    "    How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "\n",
    "\n",
    "6. TrainingArguments Configuration\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TODO\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=TODO,                # run .evaluate() every 500 steps\n",
    "    save_steps=TODO,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=TODO,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=TODO,\n",
    "    per_device_eval_batch_size=TODO,\n",
    "    num_train_epochs=TODO,\n",
    "    learning_rate=TODO,\n",
    "    weight_decay=TODO,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "    What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "\n",
    "\n",
    "7. Train & Evaluate\n",
    "\n",
    "# Train\n",
    "from transformers import Trainer\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=TODO,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#Evaluate\n",
    "metrics = TODO\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}\n",
    "\n",
    "\n",
    "    Interpret your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c3c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import transformers as tf_transformers\n",
    "import evaluate as evaluate\n",
    "import pandas as pd\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481f1ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sms': Value('string'), 'label': ClassLabel(names=['ham', 'spam'])}\n",
      "{'sms': ['sports fans - get the latest sports news str* 2 ur mobile 1 wk FREE PLUS a FREE TONE Txt SPORT ON to 8007 www.getzed.co.uk 0870141701216+ norm 4txt/120p \\n', \"It's justbeen overa week since we broke up and already our brains are going to mush!\\n\", 'Not directly behind... Abt 4 rows behind ü...\\n', 'Haha, my legs and neck are killing me and my amigos are hoping to end the night with a burn, think I could swing by in like an hour?\\n', 'Me too baby! I promise to treat you well! I bet you will take good care of me...\\n'], 'label': [1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Load the UCI SMS Spam dataset (sms_spam) from Hugging Face hub\n",
    "raw = load_dataset(\"sms_spam\", split=\"train\")\n",
    "\n",
    "# We'll use 4,000 for train, 1,000 for validation\n",
    "train_ds = raw.shuffle(seed=42).select(range(4000))\n",
    "val_ds = raw.shuffle(seed=42).select(range(4000, 5000))\n",
    "\n",
    "# print the features of the train dataset. It should show 'sms' and 'label'\n",
    "print(train_ds.features)\n",
    "# Print the first 5 rows of the train datasetprint(train_ds.features)\n",
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d6454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sms', 'labels', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    " #load the tokenize, we will use GPT2\n",
    "model_name = \"gpt2\"  # or \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer  = tf_transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "# GPT-2 has no pad token by default—set it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    # returns input_ids, attention_mask; keep max_length small for SMS\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "    # La fonction tokenize_fn retourne bien l'attention_mask car le tokenizer Hugging Face ajoute automatiquement\n",
    "    # les clés \"input_ids\" et \"attention_mask\" dans le dictionnaire de sortie lors de l'appel avec return_tensors ou par défaut.\n",
    "    # Donc, après le .map, chaque exemple dans train_tok et val_tok aura les champs \"input_ids\", \"attention_mask\" et \"labels\".\n",
    "    # Si tu veux vérifier, tu peux afficher un exemple :\n",
    "\n",
    "#apply the tokenization by loading the subset using .map function\n",
    "#apply the tokenization by loading the subset using .map function\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
    "val_tok   = val_tok.rename_column(\"label\", \"labels\")\n",
    "print(train_tok[0].keys())  # Affichera: dict_keys(['sms', 'labels', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77e2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#import GPT2ForSequenceClassification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # spam vs ham\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c625b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"], \n",
    "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
    "        \"recall\":   recall.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
    "        \"f1\":        f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    #In an imbalanced dataset like SMS spam (often more “ham” than “spam”), why is it important to track precision and recall alongside accuracy?\n",
    "    #How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "    # Dans un jeu de données déséquilibré comme les SMS spam (où il y a souvent beaucoup plus de \"ham\" que de \"spam\"), \n",
    "    # il est important de suivre la précision (precision) et le rappel (recall) en plus de l'exactitude (accuracy). \n",
    "    # L'accuracy peut être trompeuse : un modèle qui prédit toujours \"ham\" aura une haute accuracy si le spam est rare, \n",
    "    # mais il ne détectera jamais les spams (rappel faible). La précision indique la proportion de messages prédits comme spam qui \n",
    "    # sont réellement du spam, tandis que le rappel mesure la capacité du modèle à détecter tous les spams. Un modèle avec \n",
    "    # une haute accuracy mais un faible rappel sur la classe spam signifie qu'il manque beaucoup de spams, ce qui est \n",
    "    # problématique pour une application anti-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b33126",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-sms-spam\",\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=500,                # run .evaluate() every 500 steps\n",
    "    save_steps=500,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "    #What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "    # Le weight_decay (décroissance du poids) est une technique de régularisation qui pénalise les grands poids dans le modèle afin de limiter le surapprentissage (overfitting). \n",
    "    # Une valeur plus élevée de weight_decay augmente la régularisation, ce qui peut être utile si le modèle s'adapte trop aux données d'entraînement (overfit). \n",
    "    # À l'inverse, une valeur plus faible réduit la régularisation, ce qui peut être utile si le modèle sous-apprend (underfit) ou si les données sont déjà bien régularisées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4a70093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 32:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.426500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 02:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_accuracy': 0.874, 'eval_precision': 0.125, 'eval_recall': 0.008333333333333333, 'eval_f1': 0.015625, 'eval_runtime': 140.7296, 'eval_samples_per_second': 7.106, 'eval_steps_per_second': 0.888, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b2686",
   "metadata": {},
   "source": [
    "Ces résultats montrent une **accuracy élevée (0.874)**, mais une **précision (0.125)**, un **rappel (0.008)** et un **F1-score (0.016)** très faibles. Cela signifie que le modèle prédit presque toujours la classe majoritaire (\"ham\", non-spam), ce qui donne une bonne accuracy si le dataset est déséquilibré. Cependant, il détecte très peu de spams (rappel très faible) et fait beaucoup d’erreurs lorsqu’il prédit \"spam\" (précision faible). \n",
    "\n",
    "**Conclusion :**  \n",
    "Le modèle n’est pas utile pour détecter les spams. Il faut probablement :\n",
    "- Rééquilibrer les classes (oversampling/undersampling, pondération des pertes)\n",
    "- Vérifier le pipeline de tokenisation et d’entraînement\n",
    "- Ajuster les hyperparamètres ou utiliser plus de données annotées pour la classe minoritaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5949",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
