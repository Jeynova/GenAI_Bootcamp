{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c9e0c9",
   "metadata": {},
   "source": [
    "\n",
    "Fine-Tuning GPT-2 for SMS Spam\n",
    "\n",
    "Last Updated: May 10th, 2025\n",
    "\n",
    "Daily Challenge: Fine-Tuning GPT-2 for SMS Spam Classification (Legacy transformers API)\n",
    "\n",
    "\n",
    "In this daily challenge, you‚Äôll fine-tune a pre-trained GPT-2 model to classify SMS messages as spam or ham (not spam). We‚Äôll work through loading the dataset, inspecting its schema, tokenizing examples, adapting to an older transformers version, and running training and evaluation with the classic do_train/do_eval flags.\n",
    "\n",
    "\n",
    "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
    "\n",
    "    How to load and explore a custom text-classification dataset\n",
    "    Inspecting and aligning column names for tokenization\n",
    "    Tokenizing text for GPT-2 (with its peculiar padding setup)\n",
    "    Initializing GPT2ForSequenceClassification\n",
    "    Defining and computing multiple evaluation metrics\n",
    "    Configuring TrainingArguments for transformers < 4.4 (using do_train, eval_steps, etc.)\n",
    "    Running fine-tuning with Trainer and interpreting results\n",
    "    Common pitfalls when using legacy APIs\n",
    "\n",
    "\n",
    "üõ†Ô∏è What you will create\n",
    "\n",
    "By the end of this challenge, you will have built:\n",
    "\n",
    "    A tokenized SMS dataset compatible with GPT-2‚Äôs requirements, including custom padding and truncation.\n",
    "    A fine-tuned GPT2ForSequenceClassification model that can accurately label incoming SMS messages as spam or ham.\n",
    "    A complete training pipeline using the legacy do_train/do_eval flags in TrainingArguments, with periodic checkpointing, logging, and evaluation.\n",
    "    A set of evaluation metrics (accuracy, precision, recall, F1) computed at each validation step and summarized after training.\n",
    "    A reusable Jupyter notebook that ties everything together‚Äîfrom dataset loading and inspection, through model initialization and tokenization, to training, evaluation, and results interpretation.\n",
    "\n",
    "\n",
    "üíº Prerequisites\n",
    "\n",
    "    Python 3.7+\n",
    "    Installed packages: datasets, evaluate, transformers>=4.0.0,<4.4.0\n",
    "    Basic familiarity with Hugging Face‚Äôs datasets and transformers libraries\n",
    "    GitHub or Colab access for executing the notebook\n",
    "    A Hugging Face API and a WeightAndBiases API, for instructions on how to get it, click here.\n",
    "\n",
    "\n",
    "Task\n",
    "\n",
    "We will guide you through making a fine-tuning a GPT-2 model to classify SMS messages as spam or ham using an older version of transformers (<4.4). Follow the steps below and complete the ‚ÄúTODO‚Äù in the code.\n",
    "\n",
    "1. Setup : Install required packages datasets, evaluate and transformers[sentencepiece].\n",
    "\n",
    "%pip install --quiet datasets evaluate transformers[sentencepiece]\n",
    "\n",
    "\n",
    "2. Load & Inspect Dataset :\n",
    "\n",
    "from datasets import TODO #import load_dataset\n",
    "TODO # import pandas\n",
    "\n",
    "# Load the UCI SMS Spam dataset (sms_spam) from Hugging Face hub\n",
    "raw = TODO\n",
    "\n",
    "# We'll use 4,000 for train, 1,000 for validation\n",
    "train_ds = TODO\n",
    "val_ds   = TODO\n",
    "\n",
    "TODO  # print the features of the train dataset. It should show 'sms' and 'label'\n",
    "\n",
    "\n",
    "3. Tokenization :\n",
    "\n",
    "from transformers import TODO # import GPT2Tokenizer\n",
    "\n",
    "\n",
    "model_name = TODO #load the tokenize, we will use GPT2\n",
    "tokenizer  = TODO\n",
    "# GPT-2 has no pad token by default‚Äîset it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    # returns input_ids, attention_mask; keep max_length small for SMS\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_tok = TODO #apply the tokenization by loading the subset using .map function\n",
    "val_tok   = TODO #apply the tokenization by loading the subset using .map function\n",
    "\n",
    "\n",
    "4. Model Initialization\n",
    "\n",
    "import torch\n",
    "TODO  #import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained( # Load GPT-2 with sequence classification head\n",
    "    model_name,\n",
    "    num_labels=TODO,           # spam vs. ham\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "5. Metrics Definition\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = # apply the function used for accurracy but for precision\n",
    "recall    = # apply the function used for accurracy but for recall\n",
    "f1        = # apply the function used for accurracy but for F1\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"], \n",
    "        \"precision\": TODO, # apply the function used for accurracy but for precision\n",
    "        \"recall\":    TODO, # apply the function used for accurracy but for recall\n",
    "        \"f1\":        TODO # apply the function used for accurracy but for F1\n",
    "    }\n",
    "\n",
    "\n",
    "    In an imbalanced dataset like SMS spam (often more ‚Äúham‚Äù than ‚Äúspam‚Äù), why is it important to track precision and recall alongside accuracy?\n",
    "    How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "\n",
    "\n",
    "6. TrainingArguments Configuration\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TODO\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=TODO,                # run .evaluate() every 500 steps\n",
    "    save_steps=TODO,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=TODO,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=TODO,\n",
    "    per_device_eval_batch_size=TODO,\n",
    "    num_train_epochs=TODO,\n",
    "    learning_rate=TODO,\n",
    "    weight_decay=TODO,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "    What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "\n",
    "\n",
    "7. Train & Evaluate\n",
    "\n",
    "# Train\n",
    "from transformers import Trainer\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=TODO,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#Evaluate\n",
    "metrics = TODO\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}\n",
    "\n",
    "\n",
    "    Interpret your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c3c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import transformers as tf_transformers\n",
    "import evaluate as evaluate\n",
    "import pandas as pd\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481f1ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sms': Value('string'), 'label': ClassLabel(names=['ham', 'spam'])}\n",
      "{'sms': ['sports fans - get the latest sports news str* 2 ur mobile 1 wk FREE PLUS a FREE TONE Txt SPORT ON to 8007 www.getzed.co.uk 0870141701216+ norm 4txt/120p \\n', \"It's justbeen overa week since we broke up and already our brains are going to mush!\\n\", 'Not directly behind... Abt 4 rows behind √º...\\n', 'Haha, my legs and neck are killing me and my amigos are hoping to end the night with a burn, think I could swing by in like an hour?\\n', 'Me too baby! I promise to treat you well! I bet you will take good care of me...\\n'], 'label': [1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Load the UCI SMS Spam dataset (sms_spam) from Hugging Face hub\n",
    "raw = load_dataset(\"sms_spam\", split=\"train\")\n",
    "\n",
    "# We'll use 4,000 for train, 1,000 for validation\n",
    "train_ds = raw.shuffle(seed=42).select(range(4000))\n",
    "val_ds = raw.shuffle(seed=42).select(range(4000, 5000))\n",
    "\n",
    "# print the features of the train dataset. It should show 'sms' and 'label'\n",
    "print(train_ds.features)\n",
    "# Print the first 5 rows of the train datasetprint(train_ds.features)\n",
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d6454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sms', 'labels', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    " #load the tokenize, we will use GPT2\n",
    "model_name = \"gpt2\"  # or \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer  = tf_transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "# GPT-2 has no pad token by default‚Äîset it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    # returns input_ids, attention_mask; keep max_length small for SMS\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "    # La fonction tokenize_fn retourne bien l'attention_mask car le tokenizer Hugging Face ajoute automatiquement\n",
    "    # les cl√©s \"input_ids\" et \"attention_mask\" dans le dictionnaire de sortie lors de l'appel avec return_tensors ou par d√©faut.\n",
    "    # Donc, apr√®s le .map, chaque exemple dans train_tok et val_tok aura les champs \"input_ids\", \"attention_mask\" et \"labels\".\n",
    "    # Si tu veux v√©rifier, tu peux afficher un exemple¬†:\n",
    "\n",
    "#apply the tokenization by loading the subset using .map function\n",
    "#apply the tokenization by loading the subset using .map function\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_tok = train_tok.rename_column(\"label\", \"labels\")\n",
    "val_tok   = val_tok.rename_column(\"label\", \"labels\")\n",
    "print(train_tok[0].keys())  # Affichera: dict_keys(['sms', 'labels', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77e2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#import GPT2ForSequenceClassification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # spam vs ham\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c625b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"], \n",
    "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"],\n",
    "        \"recall\":   recall.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"],\n",
    "        \"f1\":        f1.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    #In an imbalanced dataset like SMS spam (often more ‚Äúham‚Äù than ‚Äúspam‚Äù), why is it important to track precision and recall alongside accuracy?\n",
    "    #How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "    # Dans un jeu de donn√©es d√©s√©quilibr√© comme les SMS spam (o√π il y a souvent beaucoup plus de \"ham\" que de \"spam\"), \n",
    "    # il est important de suivre la pr√©cision (precision) et le rappel (recall) en plus de l'exactitude (accuracy). \n",
    "    # L'accuracy peut √™tre trompeuse : un mod√®le qui pr√©dit toujours \"ham\" aura une haute accuracy si le spam est rare, \n",
    "    # mais il ne d√©tectera jamais les spams (rappel faible). La pr√©cision indique la proportion de messages pr√©dits comme spam qui \n",
    "    # sont r√©ellement du spam, tandis que le rappel mesure la capacit√© du mod√®le √† d√©tecter tous les spams. Un mod√®le avec \n",
    "    # une haute accuracy mais un faible rappel sur la classe spam signifie qu'il manque beaucoup de spams, ce qui est \n",
    "    # probl√©matique pour une application anti-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b33126",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-sms-spam\",\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=500,                # run .evaluate() every 500 steps\n",
    "    save_steps=500,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "    #What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "    # Le weight_decay (d√©croissance du poids) est une technique de r√©gularisation qui p√©nalise les grands poids dans le mod√®le afin de limiter le surapprentissage (overfitting). \n",
    "    # Une valeur plus √©lev√©e de weight_decay augmente la r√©gularisation, ce qui peut √™tre utile si le mod√®le s'adapte trop aux donn√©es d'entra√Ænement (overfit). \n",
    "    # √Ä l'inverse, une valeur plus faible r√©duit la r√©gularisation, ce qui peut √™tre utile si le mod√®le sous-apprend (underfit) ou si les donn√©es sont d√©j√† bien r√©gularis√©es.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4a70093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 32:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.426500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 02:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_accuracy': 0.874, 'eval_precision': 0.125, 'eval_recall': 0.008333333333333333, 'eval_f1': 0.015625, 'eval_runtime': 140.7296, 'eval_samples_per_second': 7.106, 'eval_steps_per_second': 0.888, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b2686",
   "metadata": {},
   "source": [
    "Ces r√©sultats montrent une **accuracy √©lev√©e (0.874)**, mais une **pr√©cision (0.125)**, un **rappel (0.008)** et un **F1-score (0.016)** tr√®s faibles. Cela signifie que le mod√®le pr√©dit presque toujours la classe majoritaire (\"ham\", non-spam), ce qui donne une bonne accuracy si le dataset est d√©s√©quilibr√©. Cependant, il d√©tecte tr√®s peu de spams (rappel tr√®s faible) et fait beaucoup d‚Äôerreurs lorsqu‚Äôil pr√©dit \"spam\" (pr√©cision faible). \n",
    "\n",
    "**Conclusion :**  \n",
    "Le mod√®le n‚Äôest pas utile pour d√©tecter les spams. Il faut probablement :\n",
    "- R√©√©quilibrer les classes (oversampling/undersampling, pond√©ration des pertes)\n",
    "- V√©rifier le pipeline de tokenisation et d‚Äôentra√Ænement\n",
    "- Ajuster les hyperparam√®tres ou utiliser plus de donn√©es annot√©es pour la classe minoritaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5949",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
