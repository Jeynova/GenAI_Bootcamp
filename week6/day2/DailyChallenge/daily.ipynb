{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c9e0c9",
   "metadata": {},
   "source": [
    "\n",
    "Fine-Tuning GPT-2 for SMS Spam\n",
    "\n",
    "Last Updated: May 10th, 2025\n",
    "\n",
    "Daily Challenge: Fine-Tuning GPT-2 for SMS Spam Classification (Legacy transformers API)\n",
    "\n",
    "\n",
    "In this daily challenge, you’ll fine-tune a pre-trained GPT-2 model to classify SMS messages as spam or ham (not spam). We’ll work through loading the dataset, inspecting its schema, tokenizing examples, adapting to an older transformers version, and running training and evaluation with the classic do_train/do_eval flags.\n",
    "\n",
    "\n",
    "👩‍🏫 👩🏿‍🏫 What You’ll learn\n",
    "\n",
    "    How to load and explore a custom text-classification dataset\n",
    "    Inspecting and aligning column names for tokenization\n",
    "    Tokenizing text for GPT-2 (with its peculiar padding setup)\n",
    "    Initializing GPT2ForSequenceClassification\n",
    "    Defining and computing multiple evaluation metrics\n",
    "    Configuring TrainingArguments for transformers < 4.4 (using do_train, eval_steps, etc.)\n",
    "    Running fine-tuning with Trainer and interpreting results\n",
    "    Common pitfalls when using legacy APIs\n",
    "\n",
    "\n",
    "🛠️ What you will create\n",
    "\n",
    "By the end of this challenge, you will have built:\n",
    "\n",
    "    A tokenized SMS dataset compatible with GPT-2’s requirements, including custom padding and truncation.\n",
    "    A fine-tuned GPT2ForSequenceClassification model that can accurately label incoming SMS messages as spam or ham.\n",
    "    A complete training pipeline using the legacy do_train/do_eval flags in TrainingArguments, with periodic checkpointing, logging, and evaluation.\n",
    "    A set of evaluation metrics (accuracy, precision, recall, F1) computed at each validation step and summarized after training.\n",
    "    A reusable Jupyter notebook that ties everything together—from dataset loading and inspection, through model initialization and tokenization, to training, evaluation, and results interpretation.\n",
    "\n",
    "\n",
    "💼 Prerequisites\n",
    "\n",
    "    Python 3.7+\n",
    "    Installed packages: datasets, evaluate, transformers>=4.0.0,<4.4.0\n",
    "    Basic familiarity with Hugging Face’s datasets and transformers libraries\n",
    "    GitHub or Colab access for executing the notebook\n",
    "    A Hugging Face API and a WeightAndBiases API, for instructions on how to get it, click here.\n",
    "\n",
    "\n",
    "Task\n",
    "\n",
    "We will guide you through making a fine-tuning a GPT-2 model to classify SMS messages as spam or ham using an older version of transformers (<4.4). Follow the steps below and complete the “TODO” in the code.\n",
    "\n",
    "1. Setup : Install required packages datasets, evaluate and transformers[sentencepiece].\n",
    "\n",
    "%pip install --quiet datasets evaluate transformers[sentencepiece]\n",
    "\n",
    "\n",
    "2. Load & Inspect Dataset :\n",
    "\n",
    "from datasets import TODO #import load_dataset\n",
    "TODO # import pandas\n",
    "\n",
    "# Load the UCI SMS Spam dataset (sms_spam) from Hugging Face hub\n",
    "raw = TODO\n",
    "\n",
    "# We'll use 4,000 for train, 1,000 for validation\n",
    "train_ds = TODO\n",
    "val_ds   = TODO\n",
    "\n",
    "TODO  # print the features of the train dataset. It should show 'sms' and 'label'\n",
    "\n",
    "\n",
    "3. Tokenization :\n",
    "\n",
    "from transformers import TODO # import GPT2Tokenizer\n",
    "\n",
    "\n",
    "model_name = TODO #load the tokenize, we will use GPT2\n",
    "tokenizer  = TODO\n",
    "# GPT-2 has no pad token by default—set it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    # returns input_ids, attention_mask; keep max_length small for SMS\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_tok = TODO #apply the tokenization by loading the subset using .map function\n",
    "val_tok   = TODO #apply the tokenization by loading the subset using .map function\n",
    "\n",
    "\n",
    "4. Model Initialization\n",
    "\n",
    "import torch\n",
    "TODO  #import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained( # Load GPT-2 with sequence classification head\n",
    "    model_name,\n",
    "    num_labels=TODO,           # spam vs. ham\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "5. Metrics Definition\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = # apply the function used for accurracy but for precision\n",
    "recall    = # apply the function used for accurracy but for recall\n",
    "f1        = # apply the function used for accurracy but for F1\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"], \n",
    "        \"precision\": TODO, # apply the function used for accurracy but for precision\n",
    "        \"recall\":    TODO, # apply the function used for accurracy but for recall\n",
    "        \"f1\":        TODO # apply the function used for accurracy but for F1\n",
    "    }\n",
    "\n",
    "\n",
    "    In an imbalanced dataset like SMS spam (often more “ham” than “spam”), why is it important to track precision and recall alongside accuracy?\n",
    "    How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "\n",
    "\n",
    "6. TrainingArguments Configuration\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TODO\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=TODO,                # run .evaluate() every 500 steps\n",
    "    save_steps=TODO,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=TODO,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=TODO,\n",
    "    per_device_eval_batch_size=TODO,\n",
    "    num_train_epochs=TODO,\n",
    "    learning_rate=TODO,\n",
    "    weight_decay=TODO,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "    What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "\n",
    "\n",
    "7. Train & Evaluate\n",
    "\n",
    "# Train\n",
    "from transformers import Trainer\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=TODO,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#Evaluate\n",
    "metrics = TODO\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}\n",
    "\n",
    "\n",
    "    Interpret your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c3c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import transformers as tf_transformers\n",
    "import evaluate as evaluate\n",
    "import pandas as pd\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import GPT2Tokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481f1ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sms': Value('string'), 'label': ClassLabel(names=['ham', 'spam'])}\n",
      "{'sms': ['sports fans - get the latest sports news str* 2 ur mobile 1 wk FREE PLUS a FREE TONE Txt SPORT ON to 8007 www.getzed.co.uk 0870141701216+ norm 4txt/120p \\n', \"It's justbeen overa week since we broke up and already our brains are going to mush!\\n\", 'Not directly behind... Abt 4 rows behind ü...\\n', 'Haha, my legs and neck are killing me and my amigos are hoping to end the night with a burn, think I could swing by in like an hour?\\n', 'Me too baby! I promise to treat you well! I bet you will take good care of me...\\n'], 'label': [1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Chargement du dataset UCI SMS Spam\n",
    "raw = load_dataset(\"sms_spam\")\n",
    "\n",
    "# Séparation en train/validation\n",
    "train_ds = raw[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "val_ds   = raw[\"train\"].shuffle(seed=42).select(range(4000, 5000))\n",
    "\n",
    "# Inspection des colonnes\n",
    "print(train_ds.features)\n",
    "# => Features({'sms': Value(dtype='string', ...), 'label': Value(dtype='int32', ...)})\n",
    "\n",
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d6454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:00<00:00, 7850.66 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 7519.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Définir le pad_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77e2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#import GPT2ForSequenceClassification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # spam (1) vs ham (0)\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c625b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=labels)[\"precision\"],\n",
    "        \"recall\":    recall.compute(predictions=preds, references=labels)[\"recall\"],\n",
    "        \"f1\":        f1.compute(predictions=preds, references=labels)[\"f1\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    #In an imbalanced dataset like SMS spam (often more “ham” than “spam”), why is it important to track precision and recall alongside accuracy?\n",
    "    #How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "    # Dans un jeu de données déséquilibré comme les SMS spam (où il y a souvent beaucoup plus de \"ham\" que de \"spam\"), \n",
    "    # il est important de suivre la précision (precision) et le rappel (recall) en plus de l'exactitude (accuracy). \n",
    "    # L'accuracy peut être trompeuse : un modèle qui prédit toujours \"ham\" aura une haute accuracy si le spam est rare, \n",
    "    # mais il ne détectera jamais les spams (rappel faible). La précision indique la proportion de messages prédits comme spam qui \n",
    "    # sont réellement du spam, tandis que le rappel mesure la capacité du modèle à détecter tous les spams. Un modèle avec \n",
    "    # une haute accuracy mais un faible rappel sur la classe spam signifie qu'il manque beaucoup de spams, ce qui est \n",
    "    # problématique pour une application anti-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b33126",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-sms-spam\",\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=500,                # run .evaluate() every 500 steps\n",
    "    save_steps=500,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "\n",
    "    #What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "    # Le weight_decay (décroissance du poids) est une technique de régularisation qui pénalise les grands poids dans le modèle afin de limiter le surapprentissage (overfitting). \n",
    "    # Une valeur plus élevée de weight_decay augmente la régularisation, ce qui peut être utile si le modèle s'adapte trop aux données d'entraînement (overfit). \n",
    "    # À l'inverse, une valeur plus faible réduit la régularisation, ce qui peut être utile si le modèle sous-apprend (underfit) ou si les données sont déjà bien régularisées.\n",
    "    #il ajoute une pénalité L2 sur les poids pour limiter l’overfitting. On choisit une valeur plus élevée (ex. 0.1) si on suspecte un surapprentissage; une valeur plus basse (ex. 0) \n",
    "    # si le modèle ne se met pas à généraliser ou si les performances varient peu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a70093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 17:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.036097556352615356, 'eval_accuracy': 0.995, 'eval_precision': 0.9831932773109243, 'eval_recall': 0.975, 'eval_f1': 0.9790794979079498, 'eval_runtime': 16.3866, 'eval_samples_per_second': 61.026, 'eval_steps_per_second': 7.628, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b2686",
   "metadata": {},
   "source": [
    "eval_loss\t0.036\tTrès faible perte sur la validation. Le modèle a bien appris à séparer les classes.\n",
    "eval_accuracy\t0.995\t99,5 % des SMS correctement classés : excellent score global.\n",
    "eval_precision\t0.983\t98,3 % des messages classés comme spam le sont vraiment. Faible taux de faux positifs.\n",
    "eval_recall\t0.975\t97,5 % des vrais spams ont été détectés. Faible taux de faux négatifs.\n",
    "eval_f1\t0.979\tBon équilibre précision / rappel. Score global solide pour la classe minoritaire (spam).\n",
    "epoch\t3\tAprès 3 passes complètes sur les données.\n",
    "eval_runtime\t16.38s\tTemps de validation (efficace).\n",
    "samples/sec\t61\tBon débit d’évaluation.\n",
    "\n",
    "Très peu de spams manqués (recall 0.975) → ton modèle est utile pour éviter des faux négatifs coûteux (des spams non filtrés).\n",
    "\n",
    "Peu de ham classés à tort en spam (précision 0.983) → peu de perturbation pour les utilisateurs (bons messages bloqués).\n",
    "\n",
    "F1 ≈ 0.98 : très bon compromis. Ton modèle généralise bien, même sur un jeu déséquilibré."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08456827",
   "metadata": {},
   "source": [
    "A faire pour le fun\n",
    "\n",
    "Visualisation des erreurs\n",
    "\n",
    "    Extrais les SMS mal classés pour repérer des motifs ambigus.\n",
    "\n",
    "    Exemple : des SMS promotionnels pas vraiment spams, ou du vocabulaire rare.\n",
    "\n",
    "Augmentation de données\n",
    "\n",
    "    Génère de faux spams pour améliorer la robustesse (ex. via paraphrases ou GPT-2 lui-même).\n",
    "\n",
    "Évaluation par seuil\n",
    "\n",
    "    Analyse les logits pour ajuster le seuil de décision, plutôt que argmax, pour contrôler le rappel.\n",
    "\n",
    "Comparaison avec un autre modèle\n",
    "\n",
    "    Essaie distilbert-base-uncased pour la même tâche : observe les différences de rapidité et performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c6f7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sms(texts):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1).numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "\n",
    "    results = []\n",
    "    for text, p, prob in zip(texts, preds, probs):\n",
    "        label = \"SPAM\" if p == 1 else \"HAM\"\n",
    "        confidence = prob[p]\n",
    "        print(f\"{label} ({confidence:.2%}) — {text}\")\n",
    "        results.append(label)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e273840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAM (73.69%) — Congratulations! You've won a free ticket. Call now!\n",
      "HAM (100.00%) — Salut, tu viens ce soir ?\n",
      "HAM — Congratulations! You've won a free ticket. Call now!\n",
      "HAM — Salut, tu viens ce soir ?\n"
     ]
    }
   ],
   "source": [
    "exemples = [\n",
    "    \"Congratulations! You've won a free ticket. Call now!\",\n",
    "    \"Salut, tu viens ce soir ?\"\n",
    "]\n",
    "\n",
    "résultats = predict_sms(exemples)\n",
    "for txt, label in zip(exemples, résultats):\n",
    "    print(f\"{label.upper()} — {txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5949",
   "metadata": {},
   "source": [
    "Si le modèle avait bien appris à détecter les spams dans le dataset de validation, mais échoue sur un message clair de spam en test manuel, cela indique une sensibilité à la tokenization hors contexte ou aux subtilités du padding.\n",
    "\n",
    "GPT-2 est un modèle auto-régressif, donc :\n",
    "\n",
    "    il n’aime pas les séquences trop tronquées ou mal paddées\n",
    "\n",
    "    il n’a pas de token de classification [CLS] (comme BERT)\n",
    "\n",
    "    le head de classification apprend à \"lire\" le dernier token utile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0fc5363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase spam depuis val_ds : Todays Voda numbers ending 1225 are selected to receive a £50award. If you have a match please call 08712300220 quoting claim code 3100 standard rates app \n",
      "\n",
      "SPAM (100.00%) — Todays Voda numbers ending 1225 are selected to receive a £50award. If you have a match please call 08712300220 quoting claim code 3100 standard rates app \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SPAM']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrait une phrase spam du set de validation utilisé à l'entraînement\n",
    "ex_spam = next(ex for ex in val_ds if ex['label'] == 1)['sms']\n",
    "print(\"Phrase spam depuis val_ds :\", ex_spam)\n",
    "\n",
    "# Prédiction\n",
    "predict_sms([ex_spam])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c27af",
   "metadata": {},
   "source": [
    "Le problème n’est pas dans :\n",
    "\n",
    "    l'entraînement du modèle\n",
    "\n",
    "    le mapping des labels\n",
    "\n",
    "    le head de classification\n",
    "\n",
    "Le problème vient du fait que :\n",
    "\n",
    "    Le message \"Congratulations! You've won a free ticket...\" est trop différent des exemples de spam vus à l’entraînement\n",
    "\n",
    "    Ou que ce message a une forme fréquente dans les données \"ham\" (par exemple des phrases en anglais avec exclamation, mais inoffensives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
