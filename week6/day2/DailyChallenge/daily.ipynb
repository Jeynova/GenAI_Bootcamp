{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c9e0c9",
   "metadata": {},
   "source": [
    "\n",
    "Fine-Tuning GPT-2 for SMS Spam\n",
    "\n",
    "Last Updated: May 10th, 2025\n",
    "\n",
    "Daily Challenge: Fine-Tuning GPT-2 for SMS Spam Classification (Legacy transformers API)\n",
    "\n",
    "\n",
    "In this daily challenge, you‚Äôll fine-tune a pre-trained GPT-2 model to classify SMS messages as spam or ham (not spam). We‚Äôll work through loading the dataset, inspecting its schema, tokenizing examples, adapting to an older transformers version, and running training and evaluation with the classic do_train/do_eval flags.\n",
    "\n",
    "\n",
    "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
    "\n",
    "    How to load and explore a custom text-classification dataset\n",
    "    Inspecting and aligning column names for tokenization\n",
    "    Tokenizing text for GPT-2 (with its peculiar padding setup)\n",
    "    Initializing GPT2ForSequenceClassification\n",
    "    Defining and computing multiple evaluation metrics\n",
    "    Configuring TrainingArguments for transformers < 4.4 (using do_train, eval_steps, etc.)\n",
    "    Running fine-tuning with Trainer and interpreting results\n",
    "    Common pitfalls when using legacy APIs\n",
    "\n",
    "\n",
    "üõ†Ô∏è What you will create\n",
    "\n",
    "By the end of this challenge, you will have built:\n",
    "\n",
    "    A tokenized SMS dataset compatible with GPT-2‚Äôs requirements, including custom padding and truncation.\n",
    "    A fine-tuned GPT2ForSequenceClassification model that can accurately label incoming SMS messages as spam or ham.\n",
    "    A complete training pipeline using the legacy do_train/do_eval flags in TrainingArguments, with periodic checkpointing, logging, and evaluation.\n",
    "    A set of evaluation metrics (accuracy, precision, recall, F1) computed at each validation step and summarized after training.\n",
    "    A reusable Jupyter notebook that ties everything together‚Äîfrom dataset loading and inspection, through model initialization and tokenization, to training, evaluation, and results interpretation.\n",
    "\n",
    "\n",
    "üíº Prerequisites\n",
    "\n",
    "    Python 3.7+\n",
    "    Installed packages: datasets, evaluate, transformers>=4.0.0,<4.4.0\n",
    "    Basic familiarity with Hugging Face‚Äôs datasets and transformers libraries\n",
    "    GitHub or Colab access for executing the notebook\n",
    "    A Hugging Face API and a WeightAndBiases API, for instructions on how to get it, click here.\n",
    "\n",
    "\n",
    "Task\n",
    "\n",
    "We will guide you through making a fine-tuning a GPT-2 model to classify SMS messages as spam or ham using an older version of transformers (<4.4). Follow the steps below and complete the ‚ÄúTODO‚Äù in the code.\n",
    "\n",
    "1. Setup : Install required packages datasets, evaluate and transformers[sentencepiece].\n",
    "\n",
    "%pip install --quiet datasets evaluate transformers[sentencepiece]\n",
    "\n",
    "\n",
    "2. Load & Inspect Dataset :\n",
    "\n",
    "from datasets import TODO #import load_dataset\n",
    "TODO # import pandas\n",
    "\n",
    "# Load the UCI SMS Spam dataset (sms_spam) from Hugging Face hub\n",
    "raw = TODO\n",
    "\n",
    "# We'll use 4,000 for train, 1,000 for validation\n",
    "train_ds = TODO\n",
    "val_ds   = TODO\n",
    "\n",
    "TODO  # print the features of the train dataset. It should show 'sms' and 'label'\n",
    "\n",
    "\n",
    "3. Tokenization :\n",
    "\n",
    "from transformers import TODO # import GPT2Tokenizer\n",
    "\n",
    "\n",
    "model_name = TODO #load the tokenize, we will use GPT2\n",
    "tokenizer  = TODO\n",
    "# GPT-2 has no pad token by default‚Äîset it to eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    # returns input_ids, attention_mask; keep max_length small for SMS\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_tok = TODO #apply the tokenization by loading the subset using .map function\n",
    "val_tok   = TODO #apply the tokenization by loading the subset using .map function\n",
    "\n",
    "\n",
    "4. Model Initialization\n",
    "\n",
    "import torch\n",
    "TODO  #import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained( # Load GPT-2 with sequence classification head\n",
    "    model_name,\n",
    "    num_labels=TODO,           # spam vs. ham\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "5. Metrics Definition\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = # apply the function used for accurracy but for precision\n",
    "recall    = # apply the function used for accurracy but for recall\n",
    "f1        = # apply the function used for accurracy but for F1\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"], \n",
    "        \"precision\": TODO, # apply the function used for accurracy but for precision\n",
    "        \"recall\":    TODO, # apply the function used for accurracy but for recall\n",
    "        \"f1\":        TODO # apply the function used for accurracy but for F1\n",
    "    }\n",
    "\n",
    "\n",
    "    In an imbalanced dataset like SMS spam (often more ‚Äúham‚Äù than ‚Äúspam‚Äù), why is it important to track precision and recall alongside accuracy?\n",
    "    How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "\n",
    "\n",
    "6. TrainingArguments Configuration\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TODO\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=TODO,                # run .evaluate() every 500 steps\n",
    "    save_steps=TODO,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=TODO,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=TODO,\n",
    "    per_device_eval_batch_size=TODO,\n",
    "    num_train_epochs=TODO,\n",
    "    learning_rate=TODO,\n",
    "    weight_decay=TODO,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "\n",
    "\n",
    "    What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "\n",
    "\n",
    "7. Train & Evaluate\n",
    "\n",
    "# Train\n",
    "from transformers import Trainer\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=TODO,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#Evaluate\n",
    "metrics = TODO\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}\n",
    "\n",
    "\n",
    "    Interpret your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c3c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import transformers as tf_transformers\n",
    "import evaluate as evaluate\n",
    "import pandas as pd\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import GPT2Tokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481f1ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sms': Value('string'), 'label': ClassLabel(names=['ham', 'spam'])}\n",
      "{'sms': ['sports fans - get the latest sports news str* 2 ur mobile 1 wk FREE PLUS a FREE TONE Txt SPORT ON to 8007 www.getzed.co.uk 0870141701216+ norm 4txt/120p \\n', \"It's justbeen overa week since we broke up and already our brains are going to mush!\\n\", 'Not directly behind... Abt 4 rows behind √º...\\n', 'Haha, my legs and neck are killing me and my amigos are hoping to end the night with a burn, think I could swing by in like an hour?\\n', 'Me too baby! I promise to treat you well! I bet you will take good care of me...\\n'], 'label': [1, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# Chargement du dataset UCI SMS Spam\n",
    "raw = load_dataset(\"sms_spam\")\n",
    "\n",
    "# S√©paration en train/validation\n",
    "train_ds = raw[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "val_ds   = raw[\"train\"].shuffle(seed=42).select(range(4000, 5000))\n",
    "\n",
    "# Inspection des colonnes\n",
    "print(train_ds.features)\n",
    "# => Features({'sms': Value(dtype='string', ...), 'label': Value(dtype='int32', ...)})\n",
    "\n",
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d6454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4000/4000 [00:00<00:00, 7850.66 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 7519.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # D√©finir le pad_token\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sms\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True)\n",
    "val_tok   = val_ds.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c77e2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#import GPT2ForSequenceClassification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,  # spam (1) vs ham (0)\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c625b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=labels)[\"precision\"],\n",
    "        \"recall\":    recall.compute(predictions=preds, references=labels)[\"recall\"],\n",
    "        \"f1\":        f1.compute(predictions=preds, references=labels)[\"f1\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    #In an imbalanced dataset like SMS spam (often more ‚Äúham‚Äù than ‚Äúspam‚Äù), why is it important to track precision and recall alongside accuracy?\n",
    "    #How would you interpret a model that achieves high accuracy but low recall on the spam class?\n",
    "    # Dans un jeu de donn√©es d√©s√©quilibr√© comme les SMS spam (o√π il y a souvent beaucoup plus de \"ham\" que de \"spam\"), \n",
    "    # il est important de suivre la pr√©cision (precision) et le rappel (recall) en plus de l'exactitude (accuracy). \n",
    "    # L'accuracy peut √™tre trompeuse : un mod√®le qui pr√©dit toujours \"ham\" aura une haute accuracy si le spam est rare, \n",
    "    # mais il ne d√©tectera jamais les spams (rappel faible). La pr√©cision indique la proportion de messages pr√©dits comme spam qui \n",
    "    # sont r√©ellement du spam, tandis que le rappel mesure la capacit√© du mod√®le √† d√©tecter tous les spams. Un mod√®le avec \n",
    "    # une haute accuracy mais un faible rappel sur la classe spam signifie qu'il manque beaucoup de spams, ce qui est \n",
    "    # probl√©matique pour une application anti-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b33126",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-sms-spam\",\n",
    "    do_train=True,                 # turn on training\n",
    "    do_eval=True,                  # turn on evaluation\n",
    "    eval_steps=500,                # run .evaluate() every 500 steps\n",
    "    save_steps=500,                # save a checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,             # log metrics every 500 steps\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    report_to=None,                # disable integrations\n",
    "    save_total_limit=1,            # only keep last checkpoint\n",
    ")\n",
    "\n",
    "    #What effect does weight_decay have during fine-tuning? When might you choose a higher or lower value?\n",
    "    # Le weight_decay (d√©croissance du poids) est une technique de r√©gularisation qui p√©nalise les grands poids dans le mod√®le afin de limiter le surapprentissage (overfitting). \n",
    "    # Une valeur plus √©lev√©e de weight_decay augmente la r√©gularisation, ce qui peut √™tre utile si le mod√®le s'adapte trop aux donn√©es d'entra√Ænement (overfit). \n",
    "    # √Ä l'inverse, une valeur plus faible r√©duit la r√©gularisation, ce qui peut √™tre utile si le mod√®le sous-apprend (underfit) ou si les donn√©es sont d√©j√† bien r√©gularis√©es.\n",
    "    #il ajoute une p√©nalit√© L2 sur les poids pour limiter l‚Äôoverfitting. On choisit une valeur plus √©lev√©e (ex. 0.1) si on suspecte un surapprentissage; une valeur plus basse (ex. 0) \n",
    "    # si le mod√®le ne se met pas √† g√©n√©raliser ou si les performances varient peu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a70093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 17:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mathi\\Downloads\\GenAI\\GenAI_Bootcamp\\nlp_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.036097556352615356, 'eval_accuracy': 0.995, 'eval_precision': 0.9831932773109243, 'eval_recall': 0.975, 'eval_f1': 0.9790794979079498, 'eval_runtime': 16.3866, 'eval_samples_per_second': 61.026, 'eval_steps_per_second': 7.628, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# you need to have your wandb api key ready to paste in the command line\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "# Expect something like: {\"eval_loss\": ..., \"eval_accuracy\": 0.98, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b2686",
   "metadata": {},
   "source": [
    "eval_loss\t0.036\tTr√®s faible perte sur la validation. Le mod√®le a bien appris √† s√©parer les classes.\n",
    "eval_accuracy\t0.995\t99,5‚ÄØ% des SMS correctement class√©s : excellent score global.\n",
    "eval_precision\t0.983\t98,3‚ÄØ% des messages class√©s comme spam le sont vraiment. Faible taux de faux positifs.\n",
    "eval_recall\t0.975\t97,5‚ÄØ% des vrais spams ont √©t√© d√©tect√©s. Faible taux de faux n√©gatifs.\n",
    "eval_f1\t0.979\tBon √©quilibre pr√©cision / rappel. Score global solide pour la classe minoritaire (spam).\n",
    "epoch\t3\tApr√®s 3 passes compl√®tes sur les donn√©es.\n",
    "eval_runtime\t16.38s\tTemps de validation (efficace).\n",
    "samples/sec\t61\tBon d√©bit d‚Äô√©valuation.\n",
    "\n",
    "Tr√®s peu de spams manqu√©s (recall 0.975) ‚Üí ton mod√®le est utile pour √©viter des faux n√©gatifs co√ªteux (des spams non filtr√©s).\n",
    "\n",
    "Peu de ham class√©s √† tort en spam (pr√©cision 0.983) ‚Üí peu de perturbation pour les utilisateurs (bons messages bloqu√©s).\n",
    "\n",
    "F1 ‚âà 0.98 : tr√®s bon compromis. Ton mod√®le g√©n√©ralise bien, m√™me sur un jeu d√©s√©quilibr√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08456827",
   "metadata": {},
   "source": [
    "A faire pour le fun\n",
    "\n",
    "Visualisation des erreurs\n",
    "\n",
    "    Extrais les SMS mal class√©s pour rep√©rer des motifs ambigus.\n",
    "\n",
    "    Exemple : des SMS promotionnels pas vraiment spams, ou du vocabulaire rare.\n",
    "\n",
    "Augmentation de donn√©es\n",
    "\n",
    "    G√©n√®re de faux spams pour am√©liorer la robustesse (ex. via paraphrases ou GPT-2 lui-m√™me).\n",
    "\n",
    "√âvaluation par seuil\n",
    "\n",
    "    Analyse les logits pour ajuster le seuil de d√©cision, plut√¥t que argmax, pour contr√¥ler le rappel.\n",
    "\n",
    "Comparaison avec un autre mod√®le\n",
    "\n",
    "    Essaie distilbert-base-uncased pour la m√™me t√¢che : observe les diff√©rences de rapidit√© et performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c6f7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sms(texts):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1).numpy()\n",
    "        preds = probs.argmax(axis=-1)\n",
    "\n",
    "    results = []\n",
    "    for text, p, prob in zip(texts, preds, probs):\n",
    "        label = \"SPAM\" if p == 1 else \"HAM\"\n",
    "        confidence = prob[p]\n",
    "        print(f\"{label} ({confidence:.2%}) ‚Äî {text}\")\n",
    "        results.append(label)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e273840c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAM (73.69%) ‚Äî Congratulations! You've won a free ticket. Call now!\n",
      "HAM (100.00%) ‚Äî Salut, tu viens ce soir ?\n",
      "HAM ‚Äî Congratulations! You've won a free ticket. Call now!\n",
      "HAM ‚Äî Salut, tu viens ce soir ?\n"
     ]
    }
   ],
   "source": [
    "exemples = [\n",
    "    \"Congratulations! You've won a free ticket. Call now!\",\n",
    "    \"Salut, tu viens ce soir ?\"\n",
    "]\n",
    "\n",
    "r√©sultats = predict_sms(exemples)\n",
    "for txt, label in zip(exemples, r√©sultats):\n",
    "    print(f\"{label.upper()} ‚Äî {txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5949",
   "metadata": {},
   "source": [
    "Si le mod√®le avait bien appris √† d√©tecter les spams dans le dataset de validation, mais √©choue sur un message clair de spam en test manuel, cela indique une sensibilit√© √† la tokenization hors contexte ou aux subtilit√©s du padding.\n",
    "\n",
    "GPT-2 est un mod√®le auto-r√©gressif, donc :\n",
    "\n",
    "    il n‚Äôaime pas les s√©quences trop tronqu√©es ou mal padd√©es\n",
    "\n",
    "    il n‚Äôa pas de token de classification [CLS] (comme BERT)\n",
    "\n",
    "    le head de classification apprend √† \"lire\" le dernier token utile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0fc5363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase spam depuis val_ds : Todays Voda numbers ending 1225 are selected to receive a ¬£50award. If you have a match please call 08712300220 quoting claim code 3100 standard rates app \n",
      "\n",
      "SPAM (100.00%) ‚Äî Todays Voda numbers ending 1225 are selected to receive a ¬£50award. If you have a match please call 08712300220 quoting claim code 3100 standard rates app \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SPAM']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrait une phrase spam du set de validation utilis√© √† l'entra√Ænement\n",
    "ex_spam = next(ex for ex in val_ds if ex['label'] == 1)['sms']\n",
    "print(\"Phrase spam depuis val_ds :\", ex_spam)\n",
    "\n",
    "# Pr√©diction\n",
    "predict_sms([ex_spam])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c27af",
   "metadata": {},
   "source": [
    "Le probl√®me n‚Äôest pas dans :\n",
    "\n",
    "    l'entra√Ænement du mod√®le\n",
    "\n",
    "    le mapping des labels\n",
    "\n",
    "    le head de classification\n",
    "\n",
    "Le probl√®me vient du fait que :\n",
    "\n",
    "    Le message \"Congratulations! You've won a free ticket...\" est trop diff√©rent des exemples de spam vus √† l‚Äôentra√Ænement\n",
    "\n",
    "    Ou que ce message a une forme fr√©quente dans les donn√©es \"ham\" (par exemple des phrases en anglais avec exclamation, mais inoffensives)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
