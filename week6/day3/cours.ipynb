{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2edfaec9",
   "metadata": {},
   "source": [
    "Cours Pédagogique : Introduction à RAG (Retrieval-Augmented Generation)\n",
    "Objectifs du cours\n",
    "\n",
    "    Comprendre le concept de RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "    Savoir comment RAG combine recherche documentaire et génération de texte\n",
    "\n",
    "    Identifier les composants clés de RAG\n",
    "\n",
    "    Reconnaître les avantages et limites de cette approche\n",
    "\n",
    "    Préparer une implémentation pratique de RAG avec Python\n",
    "\n",
    "1. Qu'est-ce que le RAG ?\n",
    "\n",
    "Le RAG est une architecture hybride en NLP qui combine deux étapes :\n",
    "\n",
    "    Retrieval (recherche) : extraire des documents pertinents d’une base externe\n",
    "\n",
    "    Generation (génération) : produire une réponse en s’appuyant sur ces documents\n",
    "\n",
    "Pourquoi RAG ?\n",
    "\n",
    "Les modèles génératifs classiques (comme GPT) sont limités par :\n",
    "\n",
    "    Des données figées (non mises à jour)\n",
    "\n",
    "    Le risque de \"hallucinations\" (faits inventés)\n",
    "\n",
    "    Une incapacité à s’adapter facilement à un domaine spécifique\n",
    "\n",
    "RAG permet de s’affranchir de ces limites en accédant dynamiquement à des connaissances à jour.\n",
    "2. Exemple simplifié : faire des cookies\n",
    "\n",
    "Question utilisateur : \"Comment faire des cookies aux pépites de chocolat ?\"\n",
    "\n",
    "    Recherche :\n",
    "\n",
    "        Le système cherche des recettes dans sa base documentaire\n",
    "\n",
    "        Il extrait par exemple : température du four, liste d’ingrédients, étapes de préparation\n",
    "\n",
    "    Génération :\n",
    "\n",
    "        Le modèle assemble ces données en une réponse complète, cohérente et contextualisée\n",
    "\n",
    "3. Composants clés de RAG\n",
    "\n",
    "    Retriever : encode la requête + documents en vecteurs, puis calcule leur similarité (ex : via FAISS, BERT)\n",
    "\n",
    "    Generator : génère une réponse à partir de la requête et des documents retrouvés (ex : GPT)\n",
    "\n",
    "    Base de connaissances externe : corpus documentaire utilisé pour la recherche (Wikipedia, articles, etc.)\n",
    "\n",
    "4. Avantages de RAG\n",
    "\n",
    "    Meilleure précision factuelle (moins d’hallucinations)\n",
    "\n",
    "    Accès à des données récentes\n",
    "\n",
    "    Adaptation à un domaine (médecine, finance, support client, etc.)\n",
    "\n",
    "5. Limites de RAG\n",
    "\n",
    "    Qualité des sources essentielle (biais ou informations obsolètes = réponses fausses)\n",
    "\n",
    "    Coût computationnel plus élevé (recherche + génération)\n",
    "\n",
    "    Latence accrue due à la phase de recherche\n",
    "\n",
    "6. RAG vs Fine-Tuning\n",
    "Critère\tFine-Tuning\tRAG\n",
    "Type de connaissance\tInterne (modèle pré-entrainé)\tExterne (documents récupérés)\n",
    "Mise à jour des données\tNécessite un nouvel entraînement\tAutomatique via la base documentaire\n",
    "Vitesse d'inférence\tRapide\tPlus lente (temps de recherche)\n",
    "Complexité système\tSimple\tPlus complexe (retriever + LLM)\n",
    "Approche hybride\n",
    "\n",
    "Il est possible de combiner les deux : fine-tuner un modèle sur un domaine + utiliser RAG pour les infos à jour.\n",
    "7. Mise en pratique : construire un RAG minimaliste\n",
    "Prérequis :\n",
    "\n",
    "    Installer Ollama (pour exécuter les modèles en local)\n",
    "\n",
    "    Télécharger deux modèles : un pour l’embedding, un pour la génération\n",
    "\n",
    "    Charger un dataset (ex : \"cat-facts.txt\")\n",
    "\n",
    "Étapes principales :\n",
    "\n",
    "    Indexation :\n",
    "\n",
    "        Générer les embeddings des documents (phrases sur les chats)\n",
    "\n",
    "        Stocker les vecteurs dans une base (liste ou vectordb)\n",
    "\n",
    "    Recherche :\n",
    "\n",
    "        Embedding de la requête utilisateur\n",
    "\n",
    "        Calcul de similarité cosinus avec les vecteurs indexés\n",
    "\n",
    "        Retourner les documents les plus proches\n",
    "\n",
    "    Génération :\n",
    "\n",
    "        Créer un prompt contenant : [documents pertinents + question utilisateur]\n",
    "\n",
    "        Appeler le LLM pour générer la réponse\n",
    "\n",
    "Exemple Python (simplifié) :\n",
    "import ollama\n",
    "\n",
    "# Exemple de fonction pour ajouter un chunk\n",
    "VECTOR_DB = []\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model='model_embedding', input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))\n",
    "8. Quiz de vérification\n",
    "\n",
    "    Quels sont les deux composants principaux de RAG ?\n",
    "\n",
    "    Comment fonctionne le retriever ?\n",
    "\n",
    "    Pourquoi RAG est-il plus adapté aux connaissances récentes ?\n",
    "\n",
    "    Citez un cas d’usage concret de RAG.\n",
    "\n",
    "9. Défi pratique\n",
    "\n",
    "Améliorer le script Python :\n",
    "\n",
    "    Ajouter un affichage formaté des documents retrouvés avec leur score de similarité\n",
    "\n",
    "    Intégrer une interface CLI pour poser des questions dynamiquement\n",
    "\n",
    "Conclusion\n",
    "\n",
    "RAG est un cadre puissant qui combine le meilleur de deux mondes : la génération fluide de texte et l’accès à des faits précis. Il est particulièrement utile pour des domaines spécifiques où la mise à jour des connaissances est critique.\n",
    "\n",
    "Pour progresser, tu peux maintenant passer à l’implémentation du mini-projet RAG avec Ollama.\n",
    "\n",
    "\n",
    "### Optimisation d'une base de données vectorielle pour la recherche\n",
    "\n",
    "Voici les principales méthodes pour optimiser une base de données vectorielle :\n",
    "\n",
    "- **Réduction de dimension** : Utiliser des techniques comme PCA ou UMAP pour réduire la taille des vecteurs tout en conservant l'information essentielle, ce qui accélère la recherche.\n",
    "- **Indexation efficace** : Employer des structures d’index spécialisées (FAISS, Annoy, HNSW) pour accélérer la recherche de voisins proches (approximate nearest neighbors).\n",
    "- **Quantification** : Appliquer la quantification des vecteurs (ex : Product Quantization) pour réduire l’espace mémoire et accélérer les calculs.\n",
    "- **Batching des requêtes** : Grouper plusieurs requêtes pour optimiser l’utilisation des ressources et réduire la latence.\n",
    "- **Filtrage préalable** : Appliquer des filtres (par métadonnées ou catégories) avant la recherche vectorielle pour réduire le nombre de candidats.\n",
    "- **Mise en cache** : Cacher les résultats des requêtes fréquentes pour éviter des calculs redondants.\n",
    "- **Sharding et parallélisation** : Distribuer la base sur plusieurs machines ou threads pour gérer de grands volumes de données et améliorer la scalabilité.\n",
    "- **Nettoyage et déduplication** : Supprimer les doublons et les vecteurs inutiles pour alléger la base et améliorer la pertinence des résultats.\n",
    "\n",
    "Ces méthodes permettent d’obtenir des recherches plus rapides, moins coûteuses en ressources, et plus pertinentes.\n",
    "\n",
    "#### Qu'est-ce que la quantification dans les bases de données vectorielles ?\n",
    "\n",
    "La **quantification** est une technique qui consiste à représenter des vecteurs de grande dimension par des versions compressées, généralement à l’aide de codes plus courts. L’objectif est de réduire la taille mémoire occupée par chaque vecteur et d’accélérer les calculs de similarité lors de la recherche.\n",
    "\n",
    "**Exemple :**\n",
    "- Avec la *Product Quantization* (PQ), chaque vecteur est découpé en sous-vecteurs, puis chaque sous-vecteur est remplacé par l’indice de son centre le plus proche dans un dictionnaire appris.\n",
    "- Au lieu de stocker des vecteurs en flottants (float32), on stocke des indices entiers, ce qui réduit fortement la mémoire utilisée.\n",
    "\n",
    "**Avantages :**\n",
    "- Permet de stocker des millions de vecteurs sur une seule machine.\n",
    "- Accélère la recherche de voisins proches grâce à des calculs simplifiés.\n",
    "\n",
    "**Limites :**\n",
    "- Légère perte de précision due à l’approximation.\n",
    "- Nécessite une phase d’apprentissage préalable pour construire les dictionnaires de quantification.\n",
    "\n",
    "La quantification est donc un compromis entre rapidité, coût mémoire et précision, très utile pour les systèmes de recherche à grande échelle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
