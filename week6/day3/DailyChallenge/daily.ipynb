{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b43be4",
   "metadata": {},
   "source": [
    "\n",
    "Build a (RAG) System\n",
    "\n",
    "Last Updated: July 10th, 2025\n",
    "\n",
    "Daily Challenge: Build a Retrieval Augmented Generation (RAG) System\n",
    "\n",
    "\n",
    "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
    "\n",
    "    Implement a Retrieval Augmented Generation (RAG) system using Langchain and Hugging Face.\n",
    "    Load and process datasets using Hugging Face datasets and Langchain HuggingFaceDatasetLoader.\n",
    "    Split documents into smaller chunks using Langchain RecursiveCharacterTextSplitter.\n",
    "    Generate text embeddings using Hugging Face sentence-transformers and Langchain HuggingFaceEmbeddings.\n",
    "    Create and utilize vector stores with Langchain FAISS for efficient document retrieval.\n",
    "    Prepare and integrate a pre-trained Language Model (LLM) from Hugging Face transformers for question answering.\n",
    "    Build a Retrieval QA Chain using Langchain RetrievalQA to answer questions based on retrieved documents.\n",
    "\n",
    "\n",
    "üõ†Ô∏è What you will create\n",
    "\n",
    "You will create a functional RAG system that can answer questions based on a dataset loaded from Hugging Face Datasets. This system will:\n",
    "\n",
    "    Load the databricks/databricks-dolly-15k dataset.\n",
    "    Index the dataset content into a vector store.\n",
    "    Utilize a pre-trained question-answering model from Hugging Face.\n",
    "    Answer user queries by retrieving relevant documents and using the LLM to generate answers.\n",
    "\n",
    "\n",
    "Mandatory : You must read this article before starting the exercise\n",
    "\n",
    "Faiss | LangChain\n",
    "\n",
    "\n",
    "Mandatory : You must watch these videos before starting the exercise\n",
    "\n",
    "\n",
    "PyTorch in 100 Seconds\n",
    "\n",
    "\n",
    "LangChain Explained in 13 Minutes\n",
    "\n",
    "\n",
    "Task\n",
    "\n",
    "Our task is to implement RAG using Langchain and Hugging Face!\n",
    "\n",
    "1. Set up your environment: : This ensures all the necessary tools are available to build the RAG system. Each library serves a specific role: Langchain handles the orchestration of components, transformers provide pre-trained models, sentence-transformers generate embeddings, datasets load sample data, and FAISS enables fast similarity searches.\n",
    "\n",
    "    Open your terminal or notebook environment.\n",
    "    Install all required libraries by running these commands:\n",
    "\n",
    "\n",
    "!pip install -q langchain\n",
    "!pip install -q torch\n",
    "!pip install -q transformers\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q faiss-cpu\n",
    "!pip install -U langchain-community\n",
    "\n",
    "\n",
    "2. Load the dataset: To provide the system with information to retrieve from, you‚Äôll load a real-world dataset. HuggingFaceDatasetLoader simplifies the process of accessing Hugging Face datasets and formatting them into documents that Langchain can process.\n",
    "\n",
    "    before loading the dataset, run :\n",
    "\n",
    "pip install -Uq datasets \n",
    "\n",
    "    Import HuggingFaceDatasetLoader from langchain.document_loaders.\n",
    "    Specify the dataset name and content column:\n",
    "\n",
    "\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"\n",
    "\n",
    "\n",
    "    Create a HuggingFaceDatasetLoader instance and load the data as documents:\n",
    "\n",
    "\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "data = loader.load()\n",
    "print(data[:2]) # Optional: Print the first 2 entries to verify loading\n",
    "\n",
    "\n",
    "3. Split the documents: Language models have a limit on how much text they can process at once. Splitting large documents into smaller, overlapping chunks ensures that no important context is lost and that each piece of text is a manageable size for embedding and retrieval.\n",
    "\n",
    "    Import RecursiveCharacterTextSplitter from langchain.text_splitter.\n",
    "    Create a RecursiveCharacterTextSplitter instance with a chunk_size of 1000 and chunk_overlap of 150:\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "\n",
    "    Split the loaded documents:\n",
    "\n",
    "\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(docs[0]) # Optional: Print the first document chunk\n",
    "\n",
    "\n",
    "4. Embed the text: Text needs to be converted into numerical representations (embeddings) so that similar pieces of text can be found efficiently. Using a sentence-transformer model creates embeddings that capture semantic meaning, enabling effective retrieval later.\n",
    "\n",
    "    Import HuggingFaceEmbeddings from langchain.embeddings.\n",
    "    Define the model path, model configurations, and encoding options:\n",
    "\n",
    "\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "\n",
    "    Initialize HuggingFaceEmbeddings:\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=modelPath,\n",
    "  model_kwargs=model_kwargs,\n",
    "  encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "    (Optional) Test embedding creation:\n",
    "\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(query_result[:3])\n",
    "\n",
    "\n",
    "5. Create a vector store: A vector store like FAISS indexes the embeddings, allowing fast and scalable similarity searches. This is how the system quickly finds relevant pieces of text when a query is made.\n",
    "\n",
    "    Import FAISS from langchain.vectorstores.\n",
    "    Create a FAISS vector store from the document chunks and embeddings:\n",
    "\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "    Note: This step might take some time depending on your dataset size.\n",
    "\n",
    "\n",
    "6. Prepare the LLM model: The Language Model is responsible for generating answers based on retrieved documents. Loading a pre-trained model and wrapping it in a Langchain pipeline makes it easy to integrate with the retrieval system.\n",
    "\n",
    "    Import necessary classes from transformers and langchain:\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "\n",
    "    Load the tokenizer and question-answering model:\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n",
    "\n",
    "\n",
    "    Create a question-answering pipeline:\n",
    "\n",
    "\n",
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "Youtubeer = pipeline(\n",
    "  \"question-answering\",\n",
    "  model=model_name,\n",
    "  tokenizer=tokenizer,\n",
    "  return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "    Create a Langchain pipeline wrapper:\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "  pipeline=Youtubeer,\n",
    "  model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "\n",
    "7. Build the Retrieval QA Chain: The Retrieval QA Chain connects the retriever (which finds relevant documents) with the LLM (which generates answers). This chain enables the full RAG process, where the system retrieves helpful context and then answers the user‚Äôs query based on that context.\n",
    "\n",
    "    Import RetrievalQA from langchain.chains.\n",
    "    Create a retriever from your FAISS database:\n",
    "\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4}) # Optional: You can adjust k for number of documents retrieved\n",
    "\n",
    "\n",
    "    Build the RetrievalQA chain:\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)\n",
    "\n",
    "\n",
    "8. Test your RAG system: Running a test query allows you to verify that all components are working together. This step ensures that documents are retrieved correctly and that the model generates meaningful answers based on the retrieved context.\n",
    "\n",
    "    Define your question:\n",
    "\n",
    "\n",
    "question = \"What is cheesemaking?\"\n",
    "\n",
    "\n",
    "    Run the QA chain and print the result:\n",
    "\n",
    "\n",
    "result = qa.run({\"query\": question})\n",
    "print(result) # Or print(result[\"result\"]) if the output is a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e77a8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA available: False\n",
      "FAISS vector store will use CPU backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "print(f\"Torch CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"FAISS vector store will use CPU backend.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa50f7",
   "metadata": {},
   "source": [
    "On affiche la dispo du GPU pour info, mais FAISS fonctionnera en CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d077e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49184862a144f2b950af526ac73fe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mathi\\.cache\\huggingface\\hub\\datasets--databricks--databricks-dolly-15k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64c3a397ca84a0fa18edb34736b4368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9d51ea65184f8faaf4b760bbb378e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15011 documents. Example:\n",
      "page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"' metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "data = loader.load()\n",
    "print(f\"Loaded {len(data)} documents. Example:\\n{data[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc29a85",
   "metadata": {},
   "source": [
    "Le chargement Hugging Face ‚Üí Langchain fonctionne de la m√™me mani√®re quel que soit le backend FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eea8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into chunks...\n",
      "Number of chunks: 18502. Example:\n",
      "page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"' metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting documents into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Number of chunks: {len(docs)}. Example:\\n{docs[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a7547",
   "metadata": {},
   "source": [
    "Rappel : on segmente pour tenir compte de la limite de contexte du LLM.\n",
    "\n",
    "Sur un dataset volumineux, travailler avec un sous-ensemble au d√©but : data = data[:500]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence-transformer embeddings...\n",
      "Embedding test, first 3 values: [-0.038338545709848404, 0.1234646737575531, -0.028642937541007996]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelPath = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": False}\n",
    "\n",
    "print(\"Loading sentence-transformer embeddings...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Test embedding (optionnel)\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(f\"Embedding test, first 3 values: {query_result[:3]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1bea4",
   "metadata": {},
   "source": [
    "M√™me si FAISS tourne en CPU, on peut g√©n√©rer les embeddings sur GPU si dispo (pour acc√©l√©rer cette √©tape), sinon tout sur CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18967822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS vector store (CPU, may take a while)...\n",
      "FAISS index created.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building FAISS vector store (CPU, may take a while)...\")\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "print(\"FAISS index created.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c718b",
   "metadata": {},
   "source": [
    "L‚Äôindexation peut prendre quelques minutes sur le dataset complet.\n",
    "\n",
    "Pour aller plus vite pour les premiers tests, r√©duire la taille de docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0a5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading QA model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "print(\"Loading QA model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=qa_pipeline,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442c6fb",
   "metadata": {},
   "source": [
    "Le mod√®le TinyBERT reste l√©ger, parfait pour CPU ou petit GPU.\n",
    "\n",
    "Remplacer le mod√®le si plus de puissance necessaire  (attention √† la VRAM en GPU, ou √† la RAM en CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13cf3ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Retrieval QA chain...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building Retrieval QA chain...\")\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ec4ec",
   "metadata": {},
   "source": [
    "la cha√Æne orchestre la recherche et la g√©n√©ration de r√©ponse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8faba3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc03ae2a2dc49e48c2e51cba407984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mathi\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6145eacfd84490bdcc4d060bd2ea1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770801da0f845dbbf2357aa19e62862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd036ec40464a388b3d8a6d373c434e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8895dd414773414586e17bfd016a88d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to control the spoiling of milk into cheese\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "result = qa_pipeline({\n",
    "    \"question\": \"What is cheesemaking?\",\n",
    "    \"context\": \"The goal of cheese making is to control the spoiling of milk into cheese. The milk is traditionally from...\"\n",
    "})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa48638",
   "metadata": {},
   "source": [
    "Si tout s‚Äôex√©cute sans erreur, pipeline RAG sur Windows et faiss-cpu.\n",
    "\n",
    "Si la r√©ponse est incoh√©rente, v√©rifie l‚Äôinput, la d√©coupe et la version du mod√®le."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
