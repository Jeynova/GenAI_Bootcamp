{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f348d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ“š 1. Introduction : Word2Vec - Skip-Gram (avec One-Hot)\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "ğŸ‘‹ Bienvenue !\n",
    "\n",
    "ğŸ¯ Objectif :\n",
    "- CrÃ©er un Word2Vec de type Skip-Gram avec softmax\n",
    "- Utiliser one-hot encoding pour voir les mÃ©canismes internes\n",
    "- Comprendre lâ€™apprentissage de reprÃ©sentations vectorielles\n",
    "\n",
    "ğŸ’ Ã€ savoir :\n",
    "- On utilisera une phrase simple\n",
    "- Encodage one-hot Ã  la main\n",
    "- ImplÃ©mentation explicite des matrices poids\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# âœï¸ 2. Corpus de base\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "â¡ï¸ EntrÃ©e : une phrase\n",
    "â¡ï¸ Sortie : vocabulaire + indexation + paires (centre, contexte)\n",
    "\"\"\"\n",
    "\n",
    "text = \"the cat sat on the mat\"\n",
    "words = text.split()\n",
    "vocab = sorted(set(words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\n",
    "print(\"ğŸ“š Vocabulaire :\", word_to_ix)\n",
    "print(\"ğŸ§® Taille du vocabulaire :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ” 3. GÃ©nÃ©rer les paires Skip-Gram\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "ğŸ¯ Objectif : CrÃ©er les paires (input word, context word)\n",
    "\n",
    "ğŸ”§ Ã€ faire :\n",
    "- ComplÃ¨te la boucle pour que chaque mot gÃ©nÃ¨re ses voisins\n",
    "- Modifier window_size pour observer la diffÃ©rence\n",
    "\"\"\"\n",
    "\n",
    "def generate_skipgram_pairs(words, window_size=1):\n",
    "    pairs = []\n",
    "    for i, center_word in enumerate(words):\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j != i and 0 <= j < len(words):\n",
    "                context_word = words[j]\n",
    "                pairs.append((center_word, context_word))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(words, window_size=1)\n",
    "\n",
    "print(\"ğŸ“¦ Paires (centre, contexte) :\")\n",
    "for p in pairs:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ§± 4. Encodage One-Hot\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "â¡ï¸ On encode chaque mot en vecteur one-hot\n",
    "\n",
    "ğŸ”§ Ã€ faire :\n",
    "- ComplÃ¨te la fonction one_hot_encode\n",
    "- VÃ©rifie que l'encodage est correct\n",
    "\"\"\"\n",
    "\n",
    "def one_hot_encode(word, word_to_ix):\n",
    "    vector = np.zeros(len(word_to_ix))\n",
    "    index = word_to_ix[word]\n",
    "    vector[index] = 1\n",
    "    return vector\n",
    "\n",
    "# Exemple :\n",
    "print(f\"ğŸ§ª 'cat' â†’ {one_hot_encode('cat', word_to_ix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ§  5. Initialiser les poids du modÃ¨le\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "â¡ï¸ On va simuler le modÃ¨le Word2Vec :\n",
    "- 1 couche cachÃ©e : W1 (vocab_size x emb_dim)\n",
    "- 1 couche de sortie : W2 (emb_dim x vocab_size)\n",
    "\n",
    "ğŸ”§ Ã€ faire :\n",
    "- Choisir la dimension d'embedding\n",
    "- Initialiser W1 et W2 alÃ©atoirement\n",
    "\"\"\"\n",
    "\n",
    "embedding_dim = 5\n",
    "\n",
    "# ğŸ§± Matrices de poids\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "print(\"âœ… W1 shape:\", W1.shape)\n",
    "print(\"âœ… W2 shape:\", W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ”¥ 6. Fonction Softmax\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "â¡ï¸ Transforme un vecteur de scores en distribution de probabilitÃ©s\n",
    "\"\"\"\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stabilise les valeurs\n",
    "    return e_x / np.sum(e_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ§® 7. Forward pass : prÃ©dire contexte\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "â¡ï¸ ImplÃ©menter la passe avant :\n",
    "- Encode le mot central en one-hot\n",
    "- Multiplie par W1 â†’ vecteur cachÃ©\n",
    "- Multiplie par W2 â†’ scores pour tous les mots\n",
    "- Applique softmax\n",
    "\n",
    "ğŸ”§ Ã€ faire :\n",
    "- ComplÃ¨te la fonction forward\n",
    "\"\"\"\n",
    "\n",
    "def forward(word, W1, W2):\n",
    "    x = one_hot_encode(word, word_to_ix)       # One-hot\n",
    "    h = np.dot(x, W1)                          # Projection en espace dense\n",
    "    u = np.dot(h, W2)                          # Scores bruts\n",
    "    y_pred = softmax(u)                        # ProbabilitÃ©s\n",
    "    return y_pred, h\n",
    "\n",
    "# Test\n",
    "probs, hidden = forward(\"cat\", W1, W2)\n",
    "print(f\"ğŸ“Š Proba prÃ©dite pour 'cat' : {np.round(probs, 3)}\")\n",
    "print(\"ğŸ§  Vecteur cachÃ© :\", hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ“ 8. Fonction de perte + backpropagation\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "â¡ï¸ ImplÃ©menter la loss + gradient descente\n",
    "\n",
    "ğŸ”§ Ã€ faire :\n",
    "- ComplÃ¨te la loss\n",
    "- Applique gradient descent sur W1 et W2\n",
    "\"\"\"\n",
    "\n",
    "def train(pairs, W1, W2, epochs=100, lr=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for center, context in pairs:\n",
    "            x = one_hot_encode(center, word_to_ix)\n",
    "            y_true = one_hot_encode(context, word_to_ix)\n",
    "\n",
    "            y_pred, h = forward(center, W1, W2)\n",
    "\n",
    "            # Loss = cross entropy\n",
    "            loss = -np.sum(y_true * np.log(y_pred + 1e-9))\n",
    "            total_loss += loss\n",
    "\n",
    "            # Backprop\n",
    "            e = y_pred - y_true             # Erreur\n",
    "            dW2 = np.outer(h, e)            # Grad sortie\n",
    "            dW1 = np.outer(x, np.dot(W2, e)) # Grad entrÃ©e\n",
    "\n",
    "            W1 -= lr * dW1\n",
    "            W2 -= lr * dW2\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} â†’ Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return W1\n",
    "\n",
    "W1_trained = train(pairs, W1, W2, epochs=100, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee641ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ“ˆ 9. Visualiser les embeddings\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "â¡ï¸ Chaque mot est une ligne de W1\n",
    "â¡ï¸ On rÃ©duit en 2D avec PCA pour visualiser\n",
    "\n",
    "ğŸ”§ Ã€ faire :\n",
    "- Lancer le plot\n",
    "\"\"\"\n",
    "\n",
    "def plot_embeddings(W1, ix_to_word):\n",
    "    words = list(ix_to_word.values())\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(W1)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1])\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (reduced[i, 0], reduced[i, 1]))\n",
    "    plt.title(\"ğŸ§­ Word Embeddings (PCA)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(W1_trained, ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273012a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ğŸ¤” 10. RÃ©flexion & ComprÃ©hension\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "ğŸ§  Questions Ã  rÃ©flÃ©chir :\n",
    "- Est-ce que les mots similaires sont proches ?\n",
    "- Est-ce que le contexte influence fortement les vecteurs ?\n",
    "- Quelle est la diffÃ©rence si tu augmentes le corpus ?\n",
    "- Comment ce modÃ¨le apprend Ã  \"regrouper\" les idÃ©es ?\n",
    "\n",
    "âœï¸ Notes perso :\n",
    "- ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02a9ba",
   "metadata": {},
   "source": [
    "Ce que tu vas apprendre en le remplissant :\n",
    "Ã‰tape\tCe que tu comprends\n",
    "One-hot\tStructure du vocabulaire\n",
    "W1 / W2\tMatrices dâ€™apprentissage\n",
    "Softmax\tInterprÃ©tation des scores\n",
    "Cross Entropy\tApprentissage supervisÃ©\n",
    "Embeddings\tReprÃ©sentation des mots\n",
    "PCA\tVisualisation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
