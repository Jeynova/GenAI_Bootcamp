{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f348d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 📚 1. Introduction : Word2Vec - Skip-Gram (avec One-Hot)\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "👋 Bienvenue !\n",
    "\n",
    "🎯 Objectif :\n",
    "- Créer un Word2Vec de type Skip-Gram avec softmax\n",
    "- Utiliser one-hot encoding pour voir les mécanismes internes\n",
    "- Comprendre l’apprentissage de représentations vectorielles\n",
    "\n",
    "🎒 À savoir :\n",
    "- On utilisera une phrase simple\n",
    "- Encodage one-hot à la main\n",
    "- Implémentation explicite des matrices poids\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ✏️ 2. Corpus de base\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "➡️ Entrée : une phrase\n",
    "➡️ Sortie : vocabulaire + indexation + paires (centre, contexte)\n",
    "\"\"\"\n",
    "\n",
    "text = \"the cat sat on the mat\"\n",
    "words = text.split()\n",
    "vocab = sorted(set(words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\n",
    "print(\"📚 Vocabulaire :\", word_to_ix)\n",
    "print(\"🧮 Taille du vocabulaire :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🔁 3. Générer les paires Skip-Gram\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "🎯 Objectif : Créer les paires (input word, context word)\n",
    "\n",
    "🔧 À faire :\n",
    "- Complète la boucle pour que chaque mot génère ses voisins\n",
    "- Modifier window_size pour observer la différence\n",
    "\"\"\"\n",
    "\n",
    "def generate_skipgram_pairs(words, window_size=1):\n",
    "    pairs = []\n",
    "    for i, center_word in enumerate(words):\n",
    "        for j in range(i - window_size, i + window_size + 1):\n",
    "            if j != i and 0 <= j < len(words):\n",
    "                context_word = words[j]\n",
    "                pairs.append((center_word, context_word))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(words, window_size=1)\n",
    "\n",
    "print(\"📦 Paires (centre, contexte) :\")\n",
    "for p in pairs:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🧱 4. Encodage One-Hot\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "➡️ On encode chaque mot en vecteur one-hot\n",
    "\n",
    "🔧 À faire :\n",
    "- Complète la fonction one_hot_encode\n",
    "- Vérifie que l'encodage est correct\n",
    "\"\"\"\n",
    "\n",
    "def one_hot_encode(word, word_to_ix):\n",
    "    vector = np.zeros(len(word_to_ix))\n",
    "    index = word_to_ix[word]\n",
    "    vector[index] = 1\n",
    "    return vector\n",
    "\n",
    "# Exemple :\n",
    "print(f\"🧪 'cat' → {one_hot_encode('cat', word_to_ix)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🧠 5. Initialiser les poids du modèle\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "➡️ On va simuler le modèle Word2Vec :\n",
    "- 1 couche cachée : W1 (vocab_size x emb_dim)\n",
    "- 1 couche de sortie : W2 (emb_dim x vocab_size)\n",
    "\n",
    "🔧 À faire :\n",
    "- Choisir la dimension d'embedding\n",
    "- Initialiser W1 et W2 aléatoirement\n",
    "\"\"\"\n",
    "\n",
    "embedding_dim = 5\n",
    "\n",
    "# 🧱 Matrices de poids\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "print(\"✅ W1 shape:\", W1.shape)\n",
    "print(\"✅ W2 shape:\", W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🔥 6. Fonction Softmax\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "➡️ Transforme un vecteur de scores en distribution de probabilités\n",
    "\"\"\"\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stabilise les valeurs\n",
    "    return e_x / np.sum(e_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🧮 7. Forward pass : prédire contexte\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "➡️ Implémenter la passe avant :\n",
    "- Encode le mot central en one-hot\n",
    "- Multiplie par W1 → vecteur caché\n",
    "- Multiplie par W2 → scores pour tous les mots\n",
    "- Applique softmax\n",
    "\n",
    "🔧 À faire :\n",
    "- Complète la fonction forward\n",
    "\"\"\"\n",
    "\n",
    "def forward(word, W1, W2):\n",
    "    x = one_hot_encode(word, word_to_ix)       # One-hot\n",
    "    h = np.dot(x, W1)                          # Projection en espace dense\n",
    "    u = np.dot(h, W2)                          # Scores bruts\n",
    "    y_pred = softmax(u)                        # Probabilités\n",
    "    return y_pred, h\n",
    "\n",
    "# Test\n",
    "probs, hidden = forward(\"cat\", W1, W2)\n",
    "print(f\"📊 Proba prédite pour 'cat' : {np.round(probs, 3)}\")\n",
    "print(\"🧠 Vecteur caché :\", hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🎓 8. Fonction de perte + backpropagation\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "➡️ Implémenter la loss + gradient descente\n",
    "\n",
    "🔧 À faire :\n",
    "- Complète la loss\n",
    "- Applique gradient descent sur W1 et W2\n",
    "\"\"\"\n",
    "\n",
    "def train(pairs, W1, W2, epochs=100, lr=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for center, context in pairs:\n",
    "            x = one_hot_encode(center, word_to_ix)\n",
    "            y_true = one_hot_encode(context, word_to_ix)\n",
    "\n",
    "            y_pred, h = forward(center, W1, W2)\n",
    "\n",
    "            # Loss = cross entropy\n",
    "            loss = -np.sum(y_true * np.log(y_pred + 1e-9))\n",
    "            total_loss += loss\n",
    "\n",
    "            # Backprop\n",
    "            e = y_pred - y_true             # Erreur\n",
    "            dW2 = np.outer(h, e)            # Grad sortie\n",
    "            dW1 = np.outer(x, np.dot(W2, e)) # Grad entrée\n",
    "\n",
    "            W1 -= lr * dW1\n",
    "            W2 -= lr * dW2\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} → Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return W1\n",
    "\n",
    "W1_trained = train(pairs, W1, W2, epochs=100, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee641ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 📈 9. Visualiser les embeddings\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "➡️ Chaque mot est une ligne de W1\n",
    "➡️ On réduit en 2D avec PCA pour visualiser\n",
    "\n",
    "🔧 À faire :\n",
    "- Lancer le plot\n",
    "\"\"\"\n",
    "\n",
    "def plot_embeddings(W1, ix_to_word):\n",
    "    words = list(ix_to_word.values())\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(W1)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1])\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (reduced[i, 0], reduced[i, 1]))\n",
    "    plt.title(\"🧭 Word Embeddings (PCA)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(W1_trained, ix_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273012a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 🤔 10. Réflexion & Compréhension\n",
    "# ========================================\n",
    "\n",
    "\"\"\"\n",
    "🧠 Questions à réfléchir :\n",
    "- Est-ce que les mots similaires sont proches ?\n",
    "- Est-ce que le contexte influence fortement les vecteurs ?\n",
    "- Quelle est la différence si tu augmentes le corpus ?\n",
    "- Comment ce modèle apprend à \"regrouper\" les idées ?\n",
    "\n",
    "✏️ Notes perso :\n",
    "- ...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02a9ba",
   "metadata": {},
   "source": [
    "Ce que tu vas apprendre en le remplissant :\n",
    "Étape\tCe que tu comprends\n",
    "One-hot\tStructure du vocabulaire\n",
    "W1 / W2\tMatrices d’apprentissage\n",
    "Softmax\tInterprétation des scores\n",
    "Cross Entropy\tApprentissage supervisé\n",
    "Embeddings\tReprésentation des mots\n",
    "PCA\tVisualisation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
