{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478cde72",
   "metadata": {},
   "source": [
    "\n",
    "Exercises XP Gold\n",
    "\n",
    "Last Updated: October 16th, 2024\n",
    "\n",
    "üë©‚Äçüè´ üë©üèø‚Äçüè´ What You‚Äôll learn\n",
    "\n",
    "    Perform a comparative analysis of structured and unstructured retail data.\n",
    "    Understand the processing and analysis of structured and unstructured healthcare data.\n",
    "    Gain familiarity with structured data through basic exploration.\n",
    "    Understand the challenges of working with unstructured data and identify structured elements in a public transportation dataset.\n",
    "    Generate a synthetic product catalog for an e-commerce platform using Faker.\n",
    "\n",
    "\n",
    "üõ†Ô∏è What you will create\n",
    "\n",
    "    A comparative analysis document discussing the insights from structured and unstructured retail data and the challenges in processing them.\n",
    "    Identification of structured data elements within the E-Commerce dataset.\n",
    "    Categorization of data elements in the dataset as structured or unstructured with justifications.\n",
    "    A Python script using Faker to generate a synthetic product catalog for an e-commerce platform.\n",
    "\n",
    "\n",
    "Exercise 1: Comparative Analysis of Retail Data\n",
    "\n",
    "Dataset: Use the Retail Dataset for structured data and Women‚Äôs E-Commerce Clothing Reviews for unstructured data.\n",
    "\n",
    "    Analyze the Retail Dataset focusing on sales trends, customer purchase patterns, and store performance.\n",
    "    Analyze the Clothing Reviews dataset to extract insights like predominant sentiments, frequently mentioned topics, and overall customer satisfaction.\n",
    "    Compare the insights you can derive from each dataset and discuss the challenges you faced in processing the unstructured data.\n",
    "\n",
    "\n",
    "Exercise 2: Basic Data Exploration in E-Commerce\n",
    "\n",
    "Dataset: Use the ‚ÄúE-Commerce Data‚Äù dataset.\n",
    "\n",
    "    Load the dataset using Pandas and print the first few rows to understand its structure.\n",
    "    Print basic information about the dataset, like the number of rows, columns, and column names.\n",
    "    Identify which columns in the dataset represent structured data (like numerical values, dates, fixed categories).\n",
    "    Suggest what type of unstructured data could complement this dataset for a more comprehensive analysis (e.g., customer reviews, product descriptions).\n",
    "    Discuss how this unstructured data might be used in conjunction with the structured data.\n",
    "\n",
    "\n",
    "Exercise 3: Analyzing a Public Transportation Dataset with a Focus on Data Types\n",
    "\n",
    "Dataset: Use the ‚ÄúMetro Interstate Traffic Volume‚Äù dataset.\n",
    "\n",
    "    Load the dataset and display the first few rows to get a sense of the data.\n",
    "    Identify and print the structured elements in the dataset, such as date-time, traffic volume, etc.\n",
    "    Based on your observation of the dataset, categorize the data elements as structured or unstructured. For example, consider elements like weather descriptions, date-time, and traffic volume. Explain why you categorized them as such.\n",
    "\n",
    "\n",
    "Exercise 4: Basic Data Analysis in a Movie Ratings Dataset\n",
    "\n",
    "Dataset: Use the ‚ÄúMovieLens Latest Datasets‚Äù.\n",
    "\n",
    "    Load the ‚Äòratings.csv‚Äô file from the dataset, which contains user ratings for movies, and display the first few rows.\n",
    "    Identify and list down the structured elements in the dataset, such as user IDs, movie IDs, ratings, and timestamps.\n",
    "    Explain why the data elements in the ‚Äòratings.csv‚Äô file are considered structured data.\n",
    "\n",
    "\n",
    "Exercise 5: Creating a Synthetic Product Catalog\n",
    "\n",
    "We want to generate a synthetic product catalog for an e-commerce platform using Faker.\n",
    "\n",
    "    Ensure that the Faker library is installed and imported into your Python environment.\n",
    "    Create a dataset of 500 products. Each product should have a unique ID, name, description, and price.\n",
    "    Use Pandas to create a DataFrame from the generated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700761fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features = pd.read_csv(\"Features data set.csv\")\n",
    "sales = pd.read_csv(\"sales data-set.csv\")\n",
    "stores = pd.read_csv(\"stores data-set.csv\")\n",
    "\n",
    "# merge de base pour avoir toutes les infos\n",
    "df = sales.merge(stores, on=\"Store\").merge(features, on=[\"Store\", \"Date\"])\n",
    "\n",
    "# convertir date pour regroupement\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"month\"] = df[\"Date\"].dt.month\n",
    "\n",
    "# total ventes par mois\n",
    "monthly_sales = df.groupby(\"month\")[\"Weekly_Sales\"].sum()\n",
    "print(monthly_sales)\n",
    "\n",
    "# meilleur magasin\n",
    "top_stores = df.groupby(\"Store\")[\"Weekly_Sales\"].sum().sort_values(ascending=False)\n",
    "print(top_stores.head())\n",
    "\n",
    "# Womens Clothing Reviews\n",
    "df_reviews = pd.read_csv(\"Womens Clothing E-Commerce Reviews.csv\")\n",
    "\n",
    "# verifier colonnes utiles\n",
    "print(df_reviews[[\"Rating\", \"Title\", \"Review Text\"]].head())\n",
    "\n",
    "# moyenne des notes\n",
    "print(\"note moyenne :\", df_reviews[\"Rating\"].mean())\n",
    "\n",
    "# review\n",
    "print(df_reviews[\"Review Text\"].dropna().head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9983091",
   "metadata": {},
   "source": [
    "Donnees Retail\n",
    "Fichiers :\n",
    "\n",
    "    Features data set.csv\n",
    "\n",
    "    sales data-set.csv\n",
    "\n",
    "    stores data-set.csv\n",
    "\n",
    "Possibilites :\n",
    "\n",
    "    Donnees organisees en colonnes et lignes (structurees)\n",
    "\n",
    "    Facile de trier, filtrer, regrouper\n",
    "\n",
    "    Analyse claire des ventes par semaine, par magasin, etc.\n",
    "\n",
    "    On peut voir quels magasins vendent le plus, ou quand les ventes augmentent\n",
    "\n",
    "Avantages :\n",
    "\n",
    "    Simple a charger avec Pandas\n",
    "\n",
    "    Les colonnes ont des noms explicites\n",
    "\n",
    "    Donnees deja pretes pour l'analyse\n",
    "\n",
    "Difficulte :\n",
    "\n",
    "    Les 3 fichiers doivent etre fusionnes sur plusieurs cles (ex: Store + Date)\n",
    "\n",
    "    Ca peut etre complique au debut si les noms de colonnes ne sont pas exactement les memes\n",
    "\n",
    "Donnees Reviews\n",
    "\n",
    "Fichier :\n",
    "\n",
    "    Womens Clothing E-Commerce Reviews.csv\n",
    "\n",
    "Possibilites :\n",
    "\n",
    "    Texte libre donc plus difficile a exploiter directement\n",
    "\n",
    "    Quelques colonnes structurees (ex: Rating, Age, Department Name)\n",
    "\n",
    "    On peut lire les avis et voir la note moyenne\n",
    "\n",
    "Avantages :\n",
    "\n",
    "    Infos qualitatives sur l'experience client\n",
    "\n",
    "    Donnees reelles avec langage naturel\n",
    "\n",
    "Difficulte :\n",
    "\n",
    "    Traitement du texte pas encore vu (pas de NLP)\n",
    "\n",
    "    Pas possible d'analyser les sentiments automatiquement\n",
    "\n",
    "    Certains avis contiennent de l'ironie ou du sarcasme, donc interpretation delicate\n",
    "\n",
    "    Nettoyage manuel si on veut extraire des infos\n",
    "\n",
    "Comparaison :\n",
    "\n",
    "    Retail = structure, rapide a traiter\n",
    "\n",
    "    Reviews = non structure, riche mais plus difficile\n",
    "\n",
    "    Plus simple de tirer des tendances claires sur les ventes que sur les emotions des clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cfa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chargement du fichier\n",
    "df = pd.read_csv(\"data.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# affichage des 5 premieres lignes\n",
    "print(df.head())\n",
    "\n",
    "# affichage des infos generales\n",
    "print(df.info())\n",
    "\n",
    "# miste des colonnes\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# types de donnees par colonne\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb240e9",
   "metadata": {},
   "source": [
    "J'ai charge le fichier data.csv contenant les ventes e-commerce.\n",
    "\n",
    "Voici un aper√ßu des premieres lignes :\n",
    "- Colonnes presentes : InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country\n",
    "- Total : 8 colonnes\n",
    "\n",
    "Les donnees structurees sont :\n",
    "- Quantity : valeur numerique\n",
    "- InvoiceDate : date (√† convertir)\n",
    "- UnitPrice : valeur numerique\n",
    "- CustomerID : identifiant numerique\n",
    "- Country : categorie fixe\n",
    "- StockCode : identifiant produit\n",
    "\n",
    "Les donnees moins structurees sont :\n",
    "- Description : texte libre\n",
    "- InvoiceNo : semi-structure\n",
    "\n",
    "Donnee non structuree √† ajouter :\n",
    "On pourrait ajouter :\n",
    "- des avis clients\n",
    "- des notes\n",
    "- des descriptions produit completes\n",
    "- ou des images des produits\n",
    "\n",
    "Utilite :\n",
    "Ces donnees pourraient etre croisees avec les donnees de vente :\n",
    "- Pour analyser l'impact des avis sur les ventes\n",
    "- Pour comprendre quels mots-cles dans les descriptions boostent les conversions\n",
    "- Pour ameliorer la recommandation produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Metro_Interstate_Traffic_Volume.csv\")\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86cea7",
   "metadata": {},
   "source": [
    "J'ai charge le fichier Metro_Interstate_Traffic_Volume.csv contenant les donnees de circulation.\n",
    "\n",
    "Voici un apercu des premieres lignes :\n",
    "\n",
    "    Colonnes presentes : date_time, traffic_volume, temp, rain_1h, snow_1h, clouds_all, holiday, weather_main, weather_description\n",
    "\n",
    "    Total : 9 colonnes\n",
    "\n",
    "Les donnees structurees sont :\n",
    "\n",
    "    date_time : date au format texte pouvant etre convertie en datetime\n",
    "\n",
    "    traffic_volume : valeur numerique (volume de circulation)\n",
    "\n",
    "    temp : temperature, valeur numerique\n",
    "\n",
    "    rain_1h : precipitation en mm, valeur numerique\n",
    "\n",
    "    snow_1h : neige en mm, valeur numerique\n",
    "\n",
    "    clouds_all : pourcentage de nuages, valeur numerique\n",
    "\n",
    "    holiday : nom du jour ferie ou \"None\", peu de valeurs uniques\n",
    "\n",
    "    weather_main : description courte du temps (ex : Clouds, Rain), valeurs repetees\n",
    "\n",
    "Les donnees non structurees sont :\n",
    "\n",
    "    weather_description : texte libre avec beaucoup de variations (ex : overcast clouds, light rain), necessite nettoyage\n",
    "\n",
    "Donnee non structuree a ajouter :\n",
    "On pourrait ajouter :\n",
    "\n",
    "    des images de la circulation\n",
    "\n",
    "    des tweets ou alertes trafic\n",
    "\n",
    "    des logs de GPS ou capteurs\n",
    "\n",
    "Utilite :\n",
    "Ces donnees pourraient etre croisees avec les donnees structurees pour :\n",
    "\n",
    "    mieux predire les pics de circulation\n",
    "\n",
    "    comprendre l'impact de la meteo sur le trafic\n",
    "\n",
    "    construire un modele de prediction ou de recommendation (trajet optimal, alertes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5bae94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating            timestamp\n",
      "0       1        2     3.5  2005-04-02 23:53:47\n",
      "1       1       29     3.5  2005-04-02 23:31:16\n",
      "2       1       32     3.5  2005-04-02 23:33:39\n",
      "3       1       47     3.5  2005-04-02 23:32:07\n",
      "4       1       50     3.5  2005-04-02 23:29:40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"rating.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac606ac",
   "metadata": {},
   "source": [
    "J'ai charge le fichier rating.csv contenant les notations de films.\n",
    "\n",
    "Voici un apercu des premieres lignes :\n",
    "\n",
    "    Colonnes presentes : userId, movieId, rating, timestamp\n",
    "\n",
    "    Total : 4 colonnes\n",
    "\n",
    "Les donnees structurees sont :\n",
    "\n",
    "    userId : identifiant numerique de l'utilisateur\n",
    "\n",
    "    movieId : identifiant numerique du film\n",
    "\n",
    "    rating : valeur numerique (float), note entre 0.5 et 5\n",
    "\n",
    "    timestamp : date de notation (a convertir)\n",
    "\n",
    "Ces donnees sont structurees car elles ont un format regulier, sont faciles a trier, filtrer ou regrouper, et directement exploitables pour l'analyse.\n",
    "\n",
    "Il n'y a pas de donnees non structurees dans ce fichier.\n",
    "\n",
    "Donnee non structuree a ajouter :\n",
    "On pourrait ajouter :\n",
    "\n",
    "    des critiques texte\n",
    "\n",
    "    des commentaires utilisateurs\n",
    "\n",
    "    des mots-cles associes aux films\n",
    "\n",
    "    ou des resumes de films\n",
    "\n",
    "Utilite :\n",
    "Ces donnees pourraient etre croisees avec les notes pour :\n",
    "\n",
    "    comprendre le ressenti des utilisateurs\n",
    "\n",
    "    ameliorer un systeme de recommandation\n",
    "\n",
    "    relier certaines emotions ou mots au niveau de note\n",
    "\n",
    "    analyser l'impact des resumes ou visuels sur les notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3cc6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id                     name category  \\\n",
      "0           1                     Soap    Music   \n",
      "1           2      Practical Steel Car   Beauty   \n",
      "2           3            Concrete Ball     Baby   \n",
      "3           4               Soft Shoes    Music   \n",
      "4           5  Ergonomic Granite Pizza     Kids   \n",
      "\n",
      "                                 description   price  \n",
      "0                 Intuitive dynamic protocol  443.74  \n",
      "1                Inverse asymmetric firmware   40.32  \n",
      "2  Re-contextualized zero tolerance forecast  154.83  \n",
      "3               Intuitive multimedia synergy  138.78  \n",
      "4        Secured demand-driven system engine   98.15  \n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker_commerce import Provider\n",
    "\n",
    "fake = Faker()\n",
    "fake.add_provider(Provider)\n",
    "\n",
    "products = []\n",
    "for i in range(500):\n",
    "    product = {\n",
    "        'product_id': i + 1,\n",
    "        'name': fake.unique.ecommerce_name(),\n",
    "        'category': fake.ecommerce_category(),\n",
    "        'description': fake.catch_phrase(),\n",
    "        'price': round(random.uniform(5.0, 500.0), 2)\n",
    "    }\n",
    "    products.append(product)\n",
    "\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
