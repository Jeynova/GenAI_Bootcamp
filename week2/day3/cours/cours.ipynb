{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73146e98",
   "metadata": {},
   "source": [
    "üéØ DATA PREPROCESSING ‚Äì COURS COMPLET, STRUCTUR√â ET EXPLIQU√â\n",
    "üîπ 1. Pourquoi faire du Data Preprocessing ?\n",
    "‚û§ D√©finition :\n",
    "\n",
    "C‚Äôest l‚Äô√©tape o√π on pr√©pare les donn√©es brutes pour les rendre exploitables : propres, coh√©rentes, analysables.\n",
    "Sans √ßa, ton mod√®le ou ton analyse va bosser sur des erreurs, ce qui fausse les r√©sultats.\n",
    "‚û§ Raisons concr√®tes :\n",
    "\n",
    "    Donn√©es souvent incompl√®tes (valeurs manquantes)\n",
    "\n",
    "    Donn√©es incoh√©rentes (formats, doublons, erreurs)\n",
    "\n",
    "    Donn√©es trop bruit√©es (outliers)\n",
    "\n",
    "    Structure non adapt√©e au mod√®le ou √† la visualisation\n",
    "\n",
    "    ‚ö†Ô∏è Tu ne peux pas faire d‚Äôanalyse s√©rieuse ou de mod√®le pr√©dictif sur des donn√©es corrompues.\n",
    "\n",
    "üîπ 2. Data Gathering (collecte)\n",
    "\n",
    "Tu pars de donn√©es externes (CSV, API, base SQL) ou internes (logs, fichiers m√©tiers).\n",
    "Dans le cas Titanic, tu les t√©l√©charges avec le Kaggle API.\n",
    "Code :\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "Pourquoi faire ?\n",
    "\n",
    "Avant de nettoyer, il faut d√©j√† lire les donn√©es et les visualiser.\n",
    "Tu ne peux pas nettoyer ce que tu ne comprends pas.\n",
    "üîπ 3. Data Exploration\n",
    "Objectif :\n",
    "\n",
    "Comprendre ce que contient ton dataset :\n",
    "\n",
    "    Quelles colonnes ?\n",
    "\n",
    "    Quels types (str, int, float‚Ä¶) ?\n",
    "\n",
    "    Combien de lignes ?\n",
    "\n",
    "    Des valeurs manquantes ?\n",
    "\n",
    "M√©thodes cl√©s :\n",
    "\n",
    "df.head()         # aper√ßu des premi√®res lignes\n",
    "df.info()         # types + colonnes + valeurs manquantes\n",
    "df.describe()     # stats des colonnes num√©riques\n",
    "\n",
    "Pourquoi faire ?\n",
    "\n",
    "Tu ne peux pas savoir quoi nettoyer sans avoir vu les probl√®mes dans les donn√©es.\n",
    "\n",
    "    Exemple : si tu ne regardes pas .isnull(), tu ne verras pas les champs vides.\n",
    "\n",
    "üîπ 4. Data Integration\n",
    "D√©finition :\n",
    "\n",
    "Fusionner plusieurs datasets pour enrichir les donn√©es.\n",
    "Ex : ajouter des infos d√©mographiques aux passagers Titanic.\n",
    "Code :\n",
    "\n",
    "df2 = pd.read_csv('demo.csv')\n",
    "df = pd.merge(df, df2, on='PassengerId', how='left')\n",
    "\n",
    "Pourquoi faire ?\n",
    "\n",
    "Un dataset seul est parfois pauvre en variables explicatives.\n",
    "\n",
    "    Tu ne pr√©dis pas bien un d√©c√®s si tu ne sais rien du passager‚Ä¶\n",
    "\n",
    "Param√®tre how :\n",
    "\n",
    "    'left' : tu gardes toutes les lignes de gauche (Titanic)\n",
    "\n",
    "    'inner' : tu ne gardes que les lignes avec correspondance des deux c√¥t√©s\n",
    "\n",
    "üîπ 5. Data Cleaning (nettoyage)\n",
    "5.1 ‚Äì Supprimer les doublons\n",
    "Pourquoi ?\n",
    "\n",
    "Des lignes en double faussent les stats et les mod√®les.\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Deux fois la m√™me ligne \"John Smith, 2e classe, 28 ans\", √ßa double son poids dans l‚Äôanalyse.\n",
    "5.2 ‚Äì Supprimer les colonnes inutiles\n",
    "Pourquoi ?\n",
    "\n",
    "Certaines colonnes ne servent √† rien pour l‚Äôanalyse, ou sont trop incompl√®tes.\n",
    "\n",
    "df = df.drop(['Cabin', 'Ticket'], axis=1)\n",
    "\n",
    "    Cabin a trop de valeurs manquantes. Ticket est souvent unique et peu informatif.\n",
    "\n",
    "5.3 ‚Äì Corriger les erreurs de structure\n",
    "Exemples typiques :\n",
    "\n",
    "    Date au mauvais format\n",
    "\n",
    "    Cat√©gories incoh√©rentes (\"male\", \"Male\", \"MALE\")\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Sex'] = df['Sex'].str.lower()  # uniformiser\n",
    "\n",
    "    Tu veux que toutes les donn√©es aient le m√™me format, sinon tes groupby explosent.\n",
    "\n",
    "5.4 ‚Äì G√©rer les valeurs manquantes\n",
    "√âtape 1 ‚Äì Identifier\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "    Cela te donne une vue colonne par colonne de ce qui manque.\n",
    "\n",
    "√âtape 2 ‚Äì D√©cider\n",
    "\n",
    "Tu as 3 choix :\n",
    "\n",
    "    ‚ùå Supprimer\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "Quand : si peu de lignes sont concern√©es.\n",
    "\n",
    "    ‚ûï Remplir (imputation simple)\n",
    "\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "Quand : si tu veux conserver les donn√©es et que la colonne est importante.\n",
    "\n",
    "    ü§ñ Imputation avanc√©e\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "Quand : si les valeurs manquantes d√©pendent d'autres colonnes.\n",
    "5.5 ‚Äì G√©rer les outliers (valeurs extr√™mes)\n",
    "Pourquoi ?\n",
    "\n",
    "Les valeurs trop extr√™mes biaisent les moyennes, les mod√®les et les distributions.\n",
    "M√©thode classique : IQR (Interquartile Range)\n",
    "\n",
    "Q1 = df['Age'].quantile(0.25)\n",
    "Q3 = df['Age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "borne_basse = Q1 - 1.5 * IQR\n",
    "borne_haute = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df[(df['Age'] >= borne_basse) & (df['Age'] <= borne_haute)]\n",
    "\n",
    "Interpr√©tation :\n",
    "\n",
    "    Q1 = limite basse des 25% de valeurs\n",
    "\n",
    "    Q3 = limite haute des 75%\n",
    "\n",
    "    IQR = zone normale\n",
    "\n",
    "    Tout ce qui sort de Q1 - 1.5√óIQR ou Q3 + 1.5√óIQR = suspect\n",
    "\n",
    "    \n",
    "\n",
    "üîπ R√©sum√© ‚Äì Data Cleaning (Nettoyage des donn√©es)\n",
    "1. Suppression des doublons\n",
    "\n",
    "    Pourquoi : les lignes en double faussent les statistiques et biaisent les mod√®les.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()  # V√©rifie qu‚Äôil n‚Äôen reste pas\n",
    "\n",
    "2. Suppression de colonnes inutiles\n",
    "\n",
    "    Pourquoi : certaines colonnes (ex : Cabin, Ticket) n‚Äôapportent aucune valeur pour l‚Äôanalyse.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df = df.drop(['Cabin', 'Ticket'], axis=1)\n",
    "\n",
    "3. Correction d‚Äôerreurs structurelles\n",
    "\n",
    "    Probl√®me : colonnes mal format√©es (ex : dates en string).\n",
    "\n",
    "    Objectif : homog√©n√©iser les types pour les rendre exploitables.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "4. D√©tection des valeurs manquantes\n",
    "\n",
    "    Objectif : rep√©rer les colonnes incompl√®tes.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df.isnull()          # rep√®re cellule par cellule\n",
    "df.isnull().sum()    # nombre de valeurs manquantes par colonne\n",
    "\n",
    "5. Traitement des valeurs manquantes\n",
    "‚û§ 5.1 Suppression (dropna)\n",
    "\n",
    "    Quand : si les lignes/colonnes concern√©es sont peu nombreuses et peu utiles.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df.dropna()           # supprime lignes incompl√®tes\n",
    "df.dropna(axis=1)     # supprime colonnes incompl√®tes\n",
    "\n",
    "‚û§ 5.2 Imputation simple\n",
    "\n",
    "    Quand : si on veut conserver les donn√©es.\n",
    "\n",
    "    M√©thodes : moyenne, m√©diane, valeur fixe.\n",
    "\n",
    "df.fillna(df.mean())    # remplit avec moyenne\n",
    "df.fillna(0)            # remplit avec z√©ro\n",
    "\n",
    "‚û§ 5.3 Imputation avanc√©e (vue plus tard)\n",
    "\n",
    "    M√©thodes : r√©gression, KNN, mod√®les ML\n",
    "\n",
    "    Utilit√© : quand les relations entre colonnes sont complexes.\n",
    "\n",
    "‚úÖ Quiz (corrig√© p√©dagogique)\n",
    "\n",
    "1. Pourquoi le preprocessing ?\n",
    "‚Üí C ‚Äì Pour garantir des donn√©es propres et fiables\n",
    "\n",
    "2. Explorer les donn√©es permet ?\n",
    "‚Üí C ‚Äì Comprendre structure, types, manquants\n",
    "\n",
    "3. Cl√© de fusion Titanic + autre dataset ?\n",
    "‚Üí D ‚Äì PassengerId\n",
    "\n",
    "4. √Ä quoi sert how='left' ?\n",
    "‚Üí B ‚Äì Type de jointure (garde tous les Titanic m√™me sans correspondance)\n",
    "\n",
    "5. Quand supprimer les lignes manquantes ?\n",
    "‚Üí C ‚Äì Si concentr√©es et peu nombreuses\n",
    "\n",
    "6. Quand remplir avec 0 ?\n",
    "‚Üí Si 0 a un sens m√©tier (ex : pas d‚Äôenfants √† bord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae44813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les donn√©es Titanic\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005c52dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris    male      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female      1      0   \n",
      "2                             Heikkinen, Miss. Laina  female      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female      1      0   \n",
      "4                           Allen, Mr. William Henry    male      0      0   \n",
      "\n",
      "             Ticket     Fare Embarked  \n",
      "0         A/5 21171   7.2500        S  \n",
      "1          PC 17599  71.2833        C  \n",
      "2  STON/O2. 3101282   7.9250        S  \n",
      "3            113803  53.1000        S  \n",
      "4            373450   8.0500        S  \n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=[\"Age\",\"Cabin\"], inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9740f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant remplissage Embarked : 0\n",
      "Valeurs manquantes apr√®s : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Valeurs manquantes avant remplissage Embarked :\", df['Embarked'].isnull().sum())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "print(\"Valeurs manquantes apr√®s :\", df['Embarked'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be18332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 = 20.125, Q3 = 38.0\n",
      "IQR = 17.875\n",
      "Bornes : -6.6875 √† 64.8125\n",
      "Nb d'outliers dans Age : 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les donn√©es Titanic\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Supprimer les lignes sans valeur d'Age pour travailler proprement\n",
    "df_age = df['Age'].dropna()\n",
    "\n",
    "# 1. Quartiles\n",
    "Q1 = df_age.quantile(0.25)\n",
    "Q3 = df_age.quantile(0.75)\n",
    "\n",
    "# 2. IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Bornes\n",
    "borne_basse = Q1 - 1.5 * IQR\n",
    "borne_haute = Q3 + 1.5 * IQR\n",
    "\n",
    "# 4. D√©tection des outliers\n",
    "outliers = df[(df['Age'] < borne_basse) | (df['Age'] > borne_haute)]\n",
    "\n",
    "# Affichage\n",
    "print(f\"Q1 = {Q1}, Q3 = {Q3}\")\n",
    "print(f\"IQR = {IQR}\")\n",
    "print(f\"Bornes : {borne_basse} √† {borne_haute}\")\n",
    "print(f\"Nb d'outliers dans Age : {outliers.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf858828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Formatted Date        Summary Precip Type  Temperature (C)  \\\n",
      "0  2006-04-01 00:00:00.000 +0200  Partly Cloudy        rain         9.472222   \n",
      "1  2006-04-01 01:00:00.000 +0200  Partly Cloudy        rain         9.355556   \n",
      "2  2006-04-01 02:00:00.000 +0200  Mostly Cloudy        rain         9.377778   \n",
      "3  2006-04-01 03:00:00.000 +0200  Partly Cloudy        rain         8.288889   \n",
      "4  2006-04-01 04:00:00.000 +0200  Mostly Cloudy        rain         8.755556   \n",
      "\n",
      "   Apparent Temperature (C)  Humidity  Wind Speed (km/h)  \\\n",
      "0                  7.388889      0.89            14.1197   \n",
      "1                  7.227778      0.86            14.2646   \n",
      "2                  9.377778      0.89             3.9284   \n",
      "3                  5.944444      0.83            14.1036   \n",
      "4                  6.977778      0.83            11.0446   \n",
      "\n",
      "   Wind Bearing (degrees)  Visibility (km)  Loud Cover  Pressure (millibars)  \\\n",
      "0                   251.0          15.8263         0.0               1015.13   \n",
      "1                   259.0          15.8263         0.0               1015.63   \n",
      "2                   204.0          14.9569         0.0               1015.94   \n",
      "3                   269.0          15.8263         0.0               1016.41   \n",
      "4                   259.0          15.8263         0.0               1016.51   \n",
      "\n",
      "                       Daily Summary  \n",
      "0  Partly cloudy throughout the day.  \n",
      "1  Partly cloudy throughout the day.  \n",
      "2  Partly cloudy throughout the day.  \n",
      "3  Partly cloudy throughout the day.  \n",
      "4  Partly cloudy throughout the day.  \n",
      "Index(['Formatted Date', 'Summary', 'Precip Type', 'Temperature (C)',\n",
      "       'Apparent Temperature (C)', 'Humidity', 'Wind Speed (km/h)',\n",
      "       'Wind Bearing (degrees)', 'Visibility (km)', 'Loud Cover',\n",
      "       'Pressure (millibars)', 'Daily Summary'],\n",
      "      dtype='object')\n",
      "Formatted Date                0\n",
      "Summary                       0\n",
      "Precip Type                 517\n",
      "Temperature (C)               0\n",
      "Apparent Temperature (C)      0\n",
      "Humidity                      0\n",
      "Wind Speed (km/h)             0\n",
      "Wind Bearing (degrees)        0\n",
      "Visibility (km)               0\n",
      "Loud Cover                    0\n",
      "Pressure (millibars)          0\n",
      "Daily Summary                 0\n",
      "dtype: int64\n",
      "   Temperature (C)  Temperature_normalized\n",
      "0         9.472222                0.506975\n",
      "1         9.355556                0.505085\n",
      "2         9.377778                0.505445\n",
      "3         8.288889                0.487805\n",
      "4         8.755556                0.495365\n",
      "         PC1        PC2\n",
      "0   7.572204  64.164119\n",
      "1   7.529264  72.179519\n",
      "2  11.592634  17.266138\n",
      "3   7.632113  82.201402\n",
      "4   8.418255  72.215926\n",
      "  Precip Type  Temperature (C)\n",
      "0        rain        13.852989\n",
      "1        snow        -3.270885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv(\"weatherHistory.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['Temperature_normalized'] = scaler.fit_transform(df[['Temperature (C)']])\n",
    "print(df[['Temperature (C)', 'Temperature_normalized']].head())\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    'Temperature (C)', 'Apparent Temperature (C)', 'Humidity',\n",
    "    'Wind Speed (km/h)', 'Wind Bearing (degrees)', 'Visibility (km)',\n",
    "    'Loud Cover', 'Pressure (millibars)'\n",
    "]\n",
    "\n",
    "pca_input = df[numeric_cols].dropna()\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(pca_input)\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "print(df_pca.head())\n",
    "\n",
    "agg_temp = df.groupby('Precip Type')['Temperature (C)'].mean().reset_index()\n",
    "print(agg_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07850a1",
   "metadata": {},
   "source": [
    "üéØ Objectif de l'exercice\n",
    "\n",
    "Pr√©parer un dataset m√©t√©o pour l‚Äôanalyse en appliquant trois techniques fondamentales de transformation de donn√©es :\n",
    "\n",
    "    Normalisation (Min-Max)\n",
    "\n",
    "    R√©duction de dimension (PCA)\n",
    "\n",
    "    Agr√©gation (moyenne par cat√©gorie)\n",
    "\n",
    "üìå √âtapes et explications\n",
    "1. Chargement et exploration\n",
    "\n",
    "    On a charg√© le fichier weatherHistory.csv.\n",
    "\n",
    "    On a v√©rifi√© la structure, les colonnes, et les valeurs manquantes.\n",
    "\n",
    "    Cela permet de comprendre les variables disponibles et de pr√©parer le nettoyage.\n",
    "\n",
    "2. Normalisation de la temp√©rature\n",
    "\n",
    "    Objectif : mettre les temp√©ratures sur une √©chelle commune entre 0 et 1.\n",
    "\n",
    "    Pourquoi ?\n",
    "\n",
    "        Certaines variables (comme la temp√©rature) ont une plage de valeurs tr√®s diff√©rente d'autres (comme l'humidit√©).\n",
    "\n",
    "        La normalisation Min-Max est utile pour √©viter qu'une variable domine les autres dans les mod√®les.\n",
    "\n",
    "MinMaxScaler transforme chaque valeur en :\n",
    "x_norm = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "    R√©sultat : une nouvelle colonne Temperature_normalized utilisable pour analyse ou machine learning.\n",
    "\n",
    "3. R√©duction de dimensions avec PCA\n",
    "\n",
    "    Objectif : r√©duire un grand nombre de colonnes num√©riques en 2 colonnes principales (PC1 et PC2).\n",
    "\n",
    "    Pourquoi ?\n",
    "\n",
    "        Permet de visualiser facilement les donn√©es en 2D.\n",
    "\n",
    "        R√©duit le bruit ou la redondance entre colonnes (ex. temp√©rature r√©elle et ressentie sont tr√®s corr√©l√©es).\n",
    "\n",
    "        Gagne en performance sur des algos qui souffrent de trop de dimensions.\n",
    "\n",
    "    M√©thode : on a s√©lectionn√© les colonnes m√©t√©o principales (temp√©rature, humidit√©, vent, pression‚Ä¶), supprim√© les lignes incompl√®tes, puis appliqu√© PCA(n_components=2).\n",
    "\n",
    "    R√©sultat : un nouveau DataFrame df_pca avec les 2 composantes principales.\n",
    "\n",
    "4. Agr√©gation : temp√©rature moyenne par type de pr√©cipitation\n",
    "\n",
    "    Objectif : regrouper les lignes selon le type de pr√©cipitation (rain, snow, etc.) et calculer la temp√©rature moyenne pour chaque groupe.\n",
    "\n",
    "    Pourquoi ?\n",
    "\n",
    "        Utile pour r√©sumer un gros dataset.\n",
    "\n",
    "        Donne une vue synth√©tique des diff√©rences de temp√©rature selon les conditions m√©t√©o.\n",
    "\n",
    "    R√©sultat :\n",
    "\n",
    "Precip Type    Temperature (C)\n",
    "rain           ‚âà 13.85¬∞C\n",
    "snow           ‚âà -3.27¬∞C\n",
    "\n",
    "‚úÖ Ce qu'on a appris\n",
    "\n",
    "    Comment nettoyer et transformer des donn√©es r√©elles pour les rendre exploitables.\n",
    "\n",
    "    √Ä quoi servent concr√®tement normalisation, PCA, et agr√©gation.\n",
    "\n",
    "    Comment combiner Pandas et Scikit-learn pour du pr√©traitement efficace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
