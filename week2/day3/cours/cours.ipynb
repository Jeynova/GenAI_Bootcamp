{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73146e98",
   "metadata": {},
   "source": [
    "🎯 DATA PREPROCESSING – COURS COMPLET, STRUCTURÉ ET EXPLIQUÉ\n",
    "🔹 1. Pourquoi faire du Data Preprocessing ?\n",
    "➤ Définition :\n",
    "\n",
    "C’est l’étape où on prépare les données brutes pour les rendre exploitables : propres, cohérentes, analysables.\n",
    "Sans ça, ton modèle ou ton analyse va bosser sur des erreurs, ce qui fausse les résultats.\n",
    "➤ Raisons concrètes :\n",
    "\n",
    "    Données souvent incomplètes (valeurs manquantes)\n",
    "\n",
    "    Données incohérentes (formats, doublons, erreurs)\n",
    "\n",
    "    Données trop bruitées (outliers)\n",
    "\n",
    "    Structure non adaptée au modèle ou à la visualisation\n",
    "\n",
    "    ⚠️ Tu ne peux pas faire d’analyse sérieuse ou de modèle prédictif sur des données corrompues.\n",
    "\n",
    "🔹 2. Data Gathering (collecte)\n",
    "\n",
    "Tu pars de données externes (CSV, API, base SQL) ou internes (logs, fichiers métiers).\n",
    "Dans le cas Titanic, tu les télécharges avec le Kaggle API.\n",
    "Code :\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "Pourquoi faire ?\n",
    "\n",
    "Avant de nettoyer, il faut déjà lire les données et les visualiser.\n",
    "Tu ne peux pas nettoyer ce que tu ne comprends pas.\n",
    "🔹 3. Data Exploration\n",
    "Objectif :\n",
    "\n",
    "Comprendre ce que contient ton dataset :\n",
    "\n",
    "    Quelles colonnes ?\n",
    "\n",
    "    Quels types (str, int, float…) ?\n",
    "\n",
    "    Combien de lignes ?\n",
    "\n",
    "    Des valeurs manquantes ?\n",
    "\n",
    "Méthodes clés :\n",
    "\n",
    "df.head()         # aperçu des premières lignes\n",
    "df.info()         # types + colonnes + valeurs manquantes\n",
    "df.describe()     # stats des colonnes numériques\n",
    "\n",
    "Pourquoi faire ?\n",
    "\n",
    "Tu ne peux pas savoir quoi nettoyer sans avoir vu les problèmes dans les données.\n",
    "\n",
    "    Exemple : si tu ne regardes pas .isnull(), tu ne verras pas les champs vides.\n",
    "\n",
    "🔹 4. Data Integration\n",
    "Définition :\n",
    "\n",
    "Fusionner plusieurs datasets pour enrichir les données.\n",
    "Ex : ajouter des infos démographiques aux passagers Titanic.\n",
    "Code :\n",
    "\n",
    "df2 = pd.read_csv('demo.csv')\n",
    "df = pd.merge(df, df2, on='PassengerId', how='left')\n",
    "\n",
    "Pourquoi faire ?\n",
    "\n",
    "Un dataset seul est parfois pauvre en variables explicatives.\n",
    "\n",
    "    Tu ne prédis pas bien un décès si tu ne sais rien du passager…\n",
    "\n",
    "Paramètre how :\n",
    "\n",
    "    'left' : tu gardes toutes les lignes de gauche (Titanic)\n",
    "\n",
    "    'inner' : tu ne gardes que les lignes avec correspondance des deux côtés\n",
    "\n",
    "🔹 5. Data Cleaning (nettoyage)\n",
    "5.1 – Supprimer les doublons\n",
    "Pourquoi ?\n",
    "\n",
    "Des lignes en double faussent les stats et les modèles.\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "Exemple :\n",
    "\n",
    "Deux fois la même ligne \"John Smith, 2e classe, 28 ans\", ça double son poids dans l’analyse.\n",
    "5.2 – Supprimer les colonnes inutiles\n",
    "Pourquoi ?\n",
    "\n",
    "Certaines colonnes ne servent à rien pour l’analyse, ou sont trop incomplètes.\n",
    "\n",
    "df = df.drop(['Cabin', 'Ticket'], axis=1)\n",
    "\n",
    "    Cabin a trop de valeurs manquantes. Ticket est souvent unique et peu informatif.\n",
    "\n",
    "5.3 – Corriger les erreurs de structure\n",
    "Exemples typiques :\n",
    "\n",
    "    Date au mauvais format\n",
    "\n",
    "    Catégories incohérentes (\"male\", \"Male\", \"MALE\")\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Sex'] = df['Sex'].str.lower()  # uniformiser\n",
    "\n",
    "    Tu veux que toutes les données aient le même format, sinon tes groupby explosent.\n",
    "\n",
    "5.4 – Gérer les valeurs manquantes\n",
    "Étape 1 – Identifier\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "    Cela te donne une vue colonne par colonne de ce qui manque.\n",
    "\n",
    "Étape 2 – Décider\n",
    "\n",
    "Tu as 3 choix :\n",
    "\n",
    "    ❌ Supprimer\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "Quand : si peu de lignes sont concernées.\n",
    "\n",
    "    ➕ Remplir (imputation simple)\n",
    "\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "Quand : si tu veux conserver les données et que la colonne est importante.\n",
    "\n",
    "    🤖 Imputation avancée\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "Quand : si les valeurs manquantes dépendent d'autres colonnes.\n",
    "5.5 – Gérer les outliers (valeurs extrêmes)\n",
    "Pourquoi ?\n",
    "\n",
    "Les valeurs trop extrêmes biaisent les moyennes, les modèles et les distributions.\n",
    "Méthode classique : IQR (Interquartile Range)\n",
    "\n",
    "Q1 = df['Age'].quantile(0.25)\n",
    "Q3 = df['Age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "borne_basse = Q1 - 1.5 * IQR\n",
    "borne_haute = Q3 + 1.5 * IQR\n",
    "\n",
    "df = df[(df['Age'] >= borne_basse) & (df['Age'] <= borne_haute)]\n",
    "\n",
    "Interprétation :\n",
    "\n",
    "    Q1 = limite basse des 25% de valeurs\n",
    "\n",
    "    Q3 = limite haute des 75%\n",
    "\n",
    "    IQR = zone normale\n",
    "\n",
    "    Tout ce qui sort de Q1 - 1.5×IQR ou Q3 + 1.5×IQR = suspect\n",
    "\n",
    "    \n",
    "\n",
    "🔹 Résumé – Data Cleaning (Nettoyage des données)\n",
    "1. Suppression des doublons\n",
    "\n",
    "    Pourquoi : les lignes en double faussent les statistiques et biaisent les modèles.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()  # Vérifie qu’il n’en reste pas\n",
    "\n",
    "2. Suppression de colonnes inutiles\n",
    "\n",
    "    Pourquoi : certaines colonnes (ex : Cabin, Ticket) n’apportent aucune valeur pour l’analyse.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df = df.drop(['Cabin', 'Ticket'], axis=1)\n",
    "\n",
    "3. Correction d’erreurs structurelles\n",
    "\n",
    "    Problème : colonnes mal formatées (ex : dates en string).\n",
    "\n",
    "    Objectif : homogénéiser les types pour les rendre exploitables.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "4. Détection des valeurs manquantes\n",
    "\n",
    "    Objectif : repérer les colonnes incomplètes.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df.isnull()          # repère cellule par cellule\n",
    "df.isnull().sum()    # nombre de valeurs manquantes par colonne\n",
    "\n",
    "5. Traitement des valeurs manquantes\n",
    "➤ 5.1 Suppression (dropna)\n",
    "\n",
    "    Quand : si les lignes/colonnes concernées sont peu nombreuses et peu utiles.\n",
    "\n",
    "    Comment :\n",
    "\n",
    "df.dropna()           # supprime lignes incomplètes\n",
    "df.dropna(axis=1)     # supprime colonnes incomplètes\n",
    "\n",
    "➤ 5.2 Imputation simple\n",
    "\n",
    "    Quand : si on veut conserver les données.\n",
    "\n",
    "    Méthodes : moyenne, médiane, valeur fixe.\n",
    "\n",
    "df.fillna(df.mean())    # remplit avec moyenne\n",
    "df.fillna(0)            # remplit avec zéro\n",
    "\n",
    "➤ 5.3 Imputation avancée (vue plus tard)\n",
    "\n",
    "    Méthodes : régression, KNN, modèles ML\n",
    "\n",
    "    Utilité : quand les relations entre colonnes sont complexes.\n",
    "\n",
    "✅ Quiz (corrigé pédagogique)\n",
    "\n",
    "1. Pourquoi le preprocessing ?\n",
    "→ C – Pour garantir des données propres et fiables\n",
    "\n",
    "2. Explorer les données permet ?\n",
    "→ C – Comprendre structure, types, manquants\n",
    "\n",
    "3. Clé de fusion Titanic + autre dataset ?\n",
    "→ D – PassengerId\n",
    "\n",
    "4. À quoi sert how='left' ?\n",
    "→ B – Type de jointure (garde tous les Titanic même sans correspondance)\n",
    "\n",
    "5. Quand supprimer les lignes manquantes ?\n",
    "→ C – Si concentrées et peu nombreuses\n",
    "\n",
    "6. Quand remplir avec 0 ?\n",
    "→ Si 0 a un sens métier (ex : pas d’enfants à bord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae44813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les données Titanic\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005c52dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris    male      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female      1      0   \n",
      "2                             Heikkinen, Miss. Laina  female      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female      1      0   \n",
      "4                           Allen, Mr. William Henry    male      0      0   \n",
      "\n",
      "             Ticket     Fare Embarked  \n",
      "0         A/5 21171   7.2500        S  \n",
      "1          PC 17599  71.2833        C  \n",
      "2  STON/O2. 3101282   7.9250        S  \n",
      "3            113803  53.1000        S  \n",
      "4            373450   8.0500        S  \n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=[\"Age\",\"Cabin\"], inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9740f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant remplissage Embarked : 0\n",
      "Valeurs manquantes après : 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Valeurs manquantes avant remplissage Embarked :\", df['Embarked'].isnull().sum())\n",
    "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "print(\"Valeurs manquantes après :\", df['Embarked'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be18332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 = 20.125, Q3 = 38.0\n",
      "IQR = 17.875\n",
      "Bornes : -6.6875 à 64.8125\n",
      "Nb d'outliers dans Age : 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les données Titanic\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Supprimer les lignes sans valeur d'Age pour travailler proprement\n",
    "df_age = df['Age'].dropna()\n",
    "\n",
    "# 1. Quartiles\n",
    "Q1 = df_age.quantile(0.25)\n",
    "Q3 = df_age.quantile(0.75)\n",
    "\n",
    "# 2. IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. Bornes\n",
    "borne_basse = Q1 - 1.5 * IQR\n",
    "borne_haute = Q3 + 1.5 * IQR\n",
    "\n",
    "# 4. Détection des outliers\n",
    "outliers = df[(df['Age'] < borne_basse) | (df['Age'] > borne_haute)]\n",
    "\n",
    "# Affichage\n",
    "print(f\"Q1 = {Q1}, Q3 = {Q3}\")\n",
    "print(f\"IQR = {IQR}\")\n",
    "print(f\"Bornes : {borne_basse} à {borne_haute}\")\n",
    "print(f\"Nb d'outliers dans Age : {outliers.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf858828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Formatted Date        Summary Precip Type  Temperature (C)  \\\n",
      "0  2006-04-01 00:00:00.000 +0200  Partly Cloudy        rain         9.472222   \n",
      "1  2006-04-01 01:00:00.000 +0200  Partly Cloudy        rain         9.355556   \n",
      "2  2006-04-01 02:00:00.000 +0200  Mostly Cloudy        rain         9.377778   \n",
      "3  2006-04-01 03:00:00.000 +0200  Partly Cloudy        rain         8.288889   \n",
      "4  2006-04-01 04:00:00.000 +0200  Mostly Cloudy        rain         8.755556   \n",
      "\n",
      "   Apparent Temperature (C)  Humidity  Wind Speed (km/h)  \\\n",
      "0                  7.388889      0.89            14.1197   \n",
      "1                  7.227778      0.86            14.2646   \n",
      "2                  9.377778      0.89             3.9284   \n",
      "3                  5.944444      0.83            14.1036   \n",
      "4                  6.977778      0.83            11.0446   \n",
      "\n",
      "   Wind Bearing (degrees)  Visibility (km)  Loud Cover  Pressure (millibars)  \\\n",
      "0                   251.0          15.8263         0.0               1015.13   \n",
      "1                   259.0          15.8263         0.0               1015.63   \n",
      "2                   204.0          14.9569         0.0               1015.94   \n",
      "3                   269.0          15.8263         0.0               1016.41   \n",
      "4                   259.0          15.8263         0.0               1016.51   \n",
      "\n",
      "                       Daily Summary  \n",
      "0  Partly cloudy throughout the day.  \n",
      "1  Partly cloudy throughout the day.  \n",
      "2  Partly cloudy throughout the day.  \n",
      "3  Partly cloudy throughout the day.  \n",
      "4  Partly cloudy throughout the day.  \n",
      "Index(['Formatted Date', 'Summary', 'Precip Type', 'Temperature (C)',\n",
      "       'Apparent Temperature (C)', 'Humidity', 'Wind Speed (km/h)',\n",
      "       'Wind Bearing (degrees)', 'Visibility (km)', 'Loud Cover',\n",
      "       'Pressure (millibars)', 'Daily Summary'],\n",
      "      dtype='object')\n",
      "Formatted Date                0\n",
      "Summary                       0\n",
      "Precip Type                 517\n",
      "Temperature (C)               0\n",
      "Apparent Temperature (C)      0\n",
      "Humidity                      0\n",
      "Wind Speed (km/h)             0\n",
      "Wind Bearing (degrees)        0\n",
      "Visibility (km)               0\n",
      "Loud Cover                    0\n",
      "Pressure (millibars)          0\n",
      "Daily Summary                 0\n",
      "dtype: int64\n",
      "   Temperature (C)  Temperature_normalized\n",
      "0         9.472222                0.506975\n",
      "1         9.355556                0.505085\n",
      "2         9.377778                0.505445\n",
      "3         8.288889                0.487805\n",
      "4         8.755556                0.495365\n",
      "         PC1        PC2\n",
      "0   7.572204  64.164119\n",
      "1   7.529264  72.179519\n",
      "2  11.592634  17.266138\n",
      "3   7.632113  82.201402\n",
      "4   8.418255  72.215926\n",
      "  Precip Type  Temperature (C)\n",
      "0        rain        13.852989\n",
      "1        snow        -3.270885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv(\"weatherHistory.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['Temperature_normalized'] = scaler.fit_transform(df[['Temperature (C)']])\n",
    "print(df[['Temperature (C)', 'Temperature_normalized']].head())\n",
    "\n",
    "\n",
    "numeric_cols = [\n",
    "    'Temperature (C)', 'Apparent Temperature (C)', 'Humidity',\n",
    "    'Wind Speed (km/h)', 'Wind Bearing (degrees)', 'Visibility (km)',\n",
    "    'Loud Cover', 'Pressure (millibars)'\n",
    "]\n",
    "\n",
    "pca_input = df[numeric_cols].dropna()\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(pca_input)\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "print(df_pca.head())\n",
    "\n",
    "agg_temp = df.groupby('Precip Type')['Temperature (C)'].mean().reset_index()\n",
    "print(agg_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07850a1",
   "metadata": {},
   "source": [
    "🎯 Objectif de l'exercice\n",
    "\n",
    "Préparer un dataset météo pour l’analyse en appliquant trois techniques fondamentales de transformation de données :\n",
    "\n",
    "    Normalisation (Min-Max)\n",
    "\n",
    "    Réduction de dimension (PCA)\n",
    "\n",
    "    Agrégation (moyenne par catégorie)\n",
    "\n",
    "📌 Étapes et explications\n",
    "1. Chargement et exploration\n",
    "\n",
    "    On a chargé le fichier weatherHistory.csv.\n",
    "\n",
    "    On a vérifié la structure, les colonnes, et les valeurs manquantes.\n",
    "\n",
    "    Cela permet de comprendre les variables disponibles et de préparer le nettoyage.\n",
    "\n",
    "2. Normalisation de la température\n",
    "\n",
    "    Objectif : mettre les températures sur une échelle commune entre 0 et 1.\n",
    "\n",
    "    Pourquoi ?\n",
    "\n",
    "        Certaines variables (comme la température) ont une plage de valeurs très différente d'autres (comme l'humidité).\n",
    "\n",
    "        La normalisation Min-Max est utile pour éviter qu'une variable domine les autres dans les modèles.\n",
    "\n",
    "MinMaxScaler transforme chaque valeur en :\n",
    "x_norm = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "    Résultat : une nouvelle colonne Temperature_normalized utilisable pour analyse ou machine learning.\n",
    "\n",
    "3. Réduction de dimensions avec PCA\n",
    "\n",
    "    Objectif : réduire un grand nombre de colonnes numériques en 2 colonnes principales (PC1 et PC2).\n",
    "\n",
    "    Pourquoi ?\n",
    "\n",
    "        Permet de visualiser facilement les données en 2D.\n",
    "\n",
    "        Réduit le bruit ou la redondance entre colonnes (ex. température réelle et ressentie sont très corrélées).\n",
    "\n",
    "        Gagne en performance sur des algos qui souffrent de trop de dimensions.\n",
    "\n",
    "    Méthode : on a sélectionné les colonnes météo principales (température, humidité, vent, pression…), supprimé les lignes incomplètes, puis appliqué PCA(n_components=2).\n",
    "\n",
    "    Résultat : un nouveau DataFrame df_pca avec les 2 composantes principales.\n",
    "\n",
    "4. Agrégation : température moyenne par type de précipitation\n",
    "\n",
    "    Objectif : regrouper les lignes selon le type de précipitation (rain, snow, etc.) et calculer la température moyenne pour chaque groupe.\n",
    "\n",
    "    Pourquoi ?\n",
    "\n",
    "        Utile pour résumer un gros dataset.\n",
    "\n",
    "        Donne une vue synthétique des différences de température selon les conditions météo.\n",
    "\n",
    "    Résultat :\n",
    "\n",
    "Precip Type    Temperature (C)\n",
    "rain           ≈ 13.85°C\n",
    "snow           ≈ -3.27°C\n",
    "\n",
    "✅ Ce qu'on a appris\n",
    "\n",
    "    Comment nettoyer et transformer des données réelles pour les rendre exploitables.\n",
    "\n",
    "    À quoi servent concrètement normalisation, PCA, et agrégation.\n",
    "\n",
    "    Comment combiner Pandas et Scikit-learn pour du prétraitement efficace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
