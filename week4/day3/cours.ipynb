{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de71a1",
   "metadata": {},
   "source": [
    "ğŸ§  Feature Engineering â€” Fiche de RÃ©vision\n",
    "ğŸ“Œ DÃ©finition\n",
    "\n",
    "Le Feature Engineering consiste Ã  transformer les donnÃ©es brutes pour extraire des variables (features) plus pertinentes et plus informatives pour un modÃ¨le ML. Cela revient Ã  \"donner des super-pouvoirs\" Ã  ton modÃ¨le en le guidant vers les bons signaux.\n",
    "\n",
    "Exemple : CrÃ©er une variable \"Surface Totale\" Ã  partir de plusieurs surfaces de piÃ¨ces pour un modÃ¨le de prÃ©diction de prix de maison.\n",
    "ğŸ¯ Pourquoi câ€™est important ?\n",
    "\n",
    "    Une bonne feature > un modÃ¨le complexe mal alimentÃ©\n",
    "\n",
    "    Câ€™est l'Ã©tape qui permet aux algorithmes de capter les patterns pertinents.\n",
    "\n",
    "    Elle impacte fortement la performance, la prÃ©cision, la robustesse du modÃ¨le.\n",
    "\n",
    "ğŸ› ï¸ Techniques classiques\n",
    "1. Imputation (Traitement des valeurs manquantes)\n",
    "\n",
    "    Moyenne : df[\"col\"].fillna(df[\"col\"].mean())\n",
    "\n",
    "    Valeur la plus frÃ©quente : df[\"col\"].fillna(df[\"col\"].value_counts().idxmax())\n",
    "\n",
    "    ZÃ©ro : df.fillna(0)\n",
    "\n",
    "2. Gestion des Outliers\n",
    "\n",
    "    Suppression (risquÃ© si dataset petit)\n",
    "\n",
    "    Remplacement (imputation aprÃ¨s dÃ©tection)\n",
    "\n",
    "    Capping (ex: limiter Ã  P1/P99)\n",
    "\n",
    "    DiscrÃ©tisation : dÃ©couper les donnÃ©es continues en intervalles (binning)\n",
    "\n",
    "3. Encodage des variables catÃ©gorielles (One-hot encoding)\n",
    "\n",
    "Transforme chaque catÃ©gorie dâ€™une variable en colonne binaire (0/1).\n",
    "UtilisÃ© pour Ã©viter les biais d'ordre implicite (problÃ¨me de label encoding).\n",
    "\n",
    "pd.get_dummies(df, columns=[\"Couleur\"], prefix=[\"Color\"])\n",
    "\n",
    "âœ… Utile pour tous les modÃ¨les qui nÃ©cessitent des entrÃ©es numÃ©riques\n",
    "âœ… PrÃ©serve lâ€™indÃ©pendance des catÃ©gories\n",
    "âœ… Ã‰vite dâ€™introduire une fausse hiÃ©rarchie entre les valeurs\n",
    "â“ Mini Quiz\n",
    "\n",
    "Quel est lâ€™objectif principal du one-hot encoding ?\n",
    "âœ… ReprÃ©senter les variables catÃ©gorielles sous forme de colonnes binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180f1ad",
   "metadata": {},
   "source": [
    "ğŸ” ModÃ¨les de Classification â€” Fiche de RÃ©vision\n",
    "ğŸ Exemple utilisÃ© :\n",
    "\n",
    "Un dataset de fruits avec deux features : Poids et Couleur\n",
    "\n",
    "X = df[['Poids (g)', 'Couleur (1=Rouge, 0=Orange)']]\n",
    "y = df['Fruit']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "ğŸŒ² Random Forest\n",
    "ğŸ§  IdÃ©e\n",
    "\n",
    "Un ensemble dâ€™arbres de dÃ©cision entraÃ®nÃ©s sur des sous-Ã©chantillons du dataset â†’ vote majoritaire.\n",
    "âœ… Avantages\n",
    "\n",
    "    Rapide, robuste, peu sensible Ã  lâ€™overfitting\n",
    "\n",
    "    GÃ¨re les donnÃ©es manquantes et les features non normalisÃ©es\n",
    "\n",
    "    Fonctionne bien avec beaucoup de features\n",
    "\n",
    "ğŸ“¦ Exemple\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "ğŸ” Gradient Boosting\n",
    "ğŸ§  IdÃ©e\n",
    "\n",
    "Construction sÃ©quentielle d'arbres oÃ¹ chaque nouveau arbre corrige les erreurs du prÃ©cÃ©dent.\n",
    "âœ… Avantages\n",
    "\n",
    "    TrÃ¨s prÃ©cis\n",
    "\n",
    "    RÃ©duit le biais\n",
    "\n",
    "    Bon sur les petits datasets bien prÃ©parÃ©s\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "âš¡ XGBoost (Extreme Gradient Boosting)\n",
    "ğŸ§  IdÃ©e\n",
    "\n",
    "Version optimisÃ©e du Gradient Boosting, avec rÃ©gularisation, gestion des valeurs manquantes, parallÃ©lisme, etc.\n",
    "âœ… Avantages\n",
    "\n",
    "    Ultra performant\n",
    "\n",
    "    Efficace pour donnÃ©es structurÃ©es\n",
    "\n",
    "    Favori dans les compÃ©titions Kaggle\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "âœ´ï¸ Support Vector Classifier (SVC)\n",
    "ğŸ§  IdÃ©e\n",
    "\n",
    "Cherche la meilleure frontiÃ¨re de dÃ©cision (maximiser la marge entre classes). Peut Ãªtre non-linÃ©aire avec les kernels.\n",
    "ğŸ“Œ HyperparamÃ¨tres clÃ©s\n",
    "\n",
    "    C (rÃ©gularisation)\n",
    "\n",
    "    kernel (linear, rbf, poly)\n",
    "\n",
    "    gamma (influence dâ€™un point dâ€™apprentissage)\n",
    "\n",
    "âœ… Avantages\n",
    "\n",
    "    Excellente performance si les donnÃ©es sont bien prÃ©parÃ©es\n",
    "\n",
    "    Efficace en haute dimension\n",
    "\n",
    "    Sensible au scaling !\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6e642",
   "metadata": {},
   "source": [
    "Voici la Partie 3 : Hyperparameter Tuning â€” Fiche de RÃ©vision, centrÃ©e sur les dÃ©finitions, mÃ©thodes, et exemples pratiques Ã  retenir.\n",
    "ğŸ§ª Hyperparameter Tuning â€” Fiche de RÃ©vision\n",
    "ğŸ“Œ DÃ©finition\n",
    "\n",
    "Les hyperparamÃ¨tres sont des paramÃ¨tres fixÃ©s avant lâ€™entraÃ®nement. Contrairement aux paramÃ¨tres appris (comme les poids dans une rÃ©gression), ils contrÃ´lent le comportement de lâ€™apprentissage.\n",
    "ğŸ”§ Exemples dâ€™hyperparamÃ¨tres\n",
    "ğŸ”· Support Vector Machine (SVC)\n",
    "\n",
    "    C : contrÃ´le la rÃ©gularisation (trade-off entre marge large et erreurs)\n",
    "\n",
    "    kernel : fonction de transformation (lineaire, rbf, poly, etc.)\n",
    "\n",
    "    gamma : influence des points dâ€™apprentissage (petit = global, grand = local)\n",
    "\n",
    "ğŸ”· XGBoost\n",
    "\n",
    "    learning_rate : taux dâ€™apprentissage\n",
    "\n",
    "    n_estimators : nombre dâ€™arbres\n",
    "\n",
    "    max_depth : profondeur maximale\n",
    "\n",
    "    min_child_weight : minimum de poids par noeud\n",
    "\n",
    "    subsample : proportion dâ€™Ã©chantillons utilisÃ©e\n",
    "\n",
    "ğŸ§­ MÃ©thodes de Tuning\n",
    "ğŸ” GridSearchCV\n",
    "\n",
    "ğŸ§  Approche exhaustive : teste toutes les combinaisons possibles\n",
    "ğŸ“Œ CoÃ»t Ã©levÃ© si la grille est grande\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "\n",
    "ğŸ² RandomizedSearchCV\n",
    "\n",
    "ğŸ§  Approche plus rapide : teste un sous-ensemble alÃ©atoire de combinaisons\n",
    "âœ… Plus scalable sur gros espaces dâ€™hyperparamÃ¨tres\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": randint(2, 10)\n",
    "}\n",
    "search = RandomizedSearchCV(RandomForestClassifier(), param_dist, cv=5)\n",
    "search.fit(X, y)\n",
    "print(search.best_params_)\n",
    "\n",
    "ğŸ§  RÃ©sumÃ© Comparatif\n",
    "MÃ©thode\tStratÃ©gie\tVitesse\tPrÃ©cision potentielle\n",
    "GridSearchCV\tExhaustive\tLente\tHaute si espace rÃ©duit\n",
    "RandomizedSearchCV\tAlÃ©atoire (sampling)\tRapide\tTrÃ¨s bonne si bien configurÃ©e\n",
    "ğŸ’ª Exercice Ã  complÃ©ter\n",
    "\n",
    "Remplis les lignes manquantes pour entraÃ®ner un RandomForest avec RandomizedSearchCV :\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_cv = RandomizedSearchCV(rf, param_dist, cv=5)\n",
    "rf_cv.fit(X, y)\n",
    "\n",
    "print(\"Tuned Parameters:\", rf_cv.best_params_)\n",
    "print(\"Best score:\", rf_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b9f12",
   "metadata": {},
   "source": [
    "ğŸ”„ Cross-Validation (CV) â€” Fiche de RÃ©vision\n",
    "ğŸ“Œ DÃ©finition\n",
    "\n",
    "Le Cross-Validation est une technique dâ€™Ã©valuation qui consiste Ã  diviser les donnÃ©es en plusieurs sous-ensembles (folds) pour entraÃ®ner et tester un modÃ¨le de faÃ§on plus robuste et rÃ©aliste que le simple train/test split.\n",
    "ğŸ¯ Pourquoi câ€™est crucial ?\n",
    "\n",
    "    âœ… Mieux estimer la performance gÃ©nÃ©ralisable (non biaisÃ©e)\n",
    "\n",
    "    âœ… RÃ©duire la variance dâ€™Ã©valuation\n",
    "\n",
    "    âœ… Ã‰viter le surapprentissage (overfitting) Ã  un seul jeu de test\n",
    "\n",
    "    âœ… Ã‰valuer diffÃ©rents modÃ¨les / hyperparamÃ¨tres de faÃ§on Ã©quitable\n",
    "\n",
    "ğŸ“¦ MÃ©thode la plus courante : K-Fold Cross-Validation\n",
    "ğŸ”§ Fonctionnement\n",
    "\n",
    "    Diviser les donnÃ©es en k groupes (folds) Ã©gaux\n",
    "\n",
    "    Pour chaque fold :\n",
    "\n",
    "        EntraÃ®ner le modÃ¨le sur k-1 folds\n",
    "\n",
    "        Tester sur le fold restant\n",
    "\n",
    "    RÃ©pÃ©ter k fois\n",
    "\n",
    "    Calculer la moyenne des scores\n",
    "\n",
    "ğŸ“Œ Exemple : k=5\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Scores pour chaque fold :\", scores)\n",
    "print(\"Score moyen :\", scores.mean())\n",
    "\n",
    "âš ï¸ Attention\n",
    "\n",
    "    CV doit Ãªtre utilisÃ© avant le test final, sinon risque de fuite de donnÃ©es.\n",
    "\n",
    "    Pour un tuning fiable : intÃ©grer CV dans le GridSearch ou RandomizedSearch.\n",
    "\n",
    "ğŸ§  Bonus : Stratified K-Fold\n",
    "\n",
    "UtilisÃ© pour les problÃ¨mes de classification :\n",
    "\n",
    "    Garantit que chaque fold contient la mÃªme proportion de classes\n",
    "\n",
    "    Evite un dÃ©sÃ©quilibre classe majoritaire / minoritaire\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"Fold:\", train_index, test_index)\n",
    "\n",
    "ğŸ” Ã€ retenir pour l'entretien ou lâ€™examen :\n",
    "\n",
    "    \"Jâ€™utilise toujours le cross-validation pour mâ€™assurer que la performance du modÃ¨le est stable et gÃ©nÃ©ralisable. Jâ€™intÃ¨gre gÃ©nÃ©ralement une K-Fold CV avec k=5 ou 10 dans mes GridSearch pour le tuning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d8a96",
   "metadata": {},
   "source": [
    "âš”ï¸ Comparaison de ModÃ¨les avec Cross-Validation\n",
    "\n",
    "On va comparer trois modÃ¨les sur un mÃªme dataset (classification binaire) :\n",
    "\n",
    "    RandomForest\n",
    "\n",
    "    GradientBoosting\n",
    "\n",
    "    SVC\n",
    "\n",
    "ğŸ§ª Code complet :\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# GÃ©nÃ©rer un dataset artificiel\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "# DÃ©finir les modÃ¨les\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"SVC\": SVC()\n",
    "}\n",
    "\n",
    "# Appliquer Cross-Validation et comparer les performances\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    print(f\"{name} - Accuracy moyenne : {scores.mean():.3f} (Ã©cart-type : {scores.std():.3f})\")\n",
    "\n",
    "ğŸ’ª Exercice corrigÃ©\n",
    "\n",
    "Objectif : ComplÃ©ter un comparatif entre LogisticRegression et RandomForestClassifier sur un dataset simulÃ©, avec CV Ã  10 folds.\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# ModÃ¨les\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# CV sur 10 folds\n",
    "logreg_scores = cross_val_score(logreg, X, y, cv=10)\n",
    "rf_scores = cross_val_score(rf, X, y, cv=10)\n",
    "\n",
    "# RÃ©sultats\n",
    "print(f\"LogisticRegression - Moyenne : {logreg_scores.mean():.3f}\")\n",
    "print(f\"RandomForest - Moyenne : {rf_scores.mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
